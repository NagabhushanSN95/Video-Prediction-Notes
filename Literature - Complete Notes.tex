% Shree KRISHNAya Namaha
%! Suppress = TooLargeSection
\documentclass{article}
\title{Literature - My Notes}
\author{Nagabhushan S N}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[mathscr]{euscript}
\usepackage{fancyhdr}
\usepackage[a4paper,margin=1in,footskip=0.25in]{geometry}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
colorlinks = true,
linkcolor = black
}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\setcounter{tocdepth}{1}
\setcounter{secnumdepth}{5}
\pagestyle{fancy}
\fancyhead[LR]{}
\fancyhead[C]{\nouppercase{\leftmark}}

\begin{document}
    \maketitle
    \pdfbookmark[1]{\contentsname}{toc}
    \tableofcontents
    \newpage


    \section{Study of Subjective and Objective Quality Assessment of Video (LIVE VQA)}\label{sec:Study_of_Subjective_and_Objective_Quality_Assessment_of_Video_(LIVE_VQA)}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Kalpana Seshadrinathan, Rajiv Soundararajan, Alan Bovik, Lawrence Cormack
        \item Institutions: UT Austin
        \item Project website: \url{https://live.ece.utexas.edu/research/Quality/live_video.html}
        \item Code:
        \item Published in: TIP 2010 June
        \item Citations: 956 (As of 12-02-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Study_of_Subjective_and_Objective_Quality_Assessment_of_Video_(LIVE_VQA):introduction}
    \begin{itemize}
        \item The only reliable method to assess the video quality perceived by a human observer is to ask human subjects for their opinion, which is termed subjective video quality assessment (VQA).
        \item 10 uncompressed reference videos, 150 distorted videos, 38 subjects
    \end{itemize}

    \subsection{Details of Subjective Study}\label{subsec:Study_of_Subjective_and_Objective_Quality_Assessment_of_Video_(LIVE_VQA):subjective-study}
    \begin{itemize}
        \item Reference videos are taken from Technical University of Munich (TUM)
    \end{itemize}

    \subsection{Processing of Subjective Scores}\label{subsec:Study_of_Subjective_and_Objective_Quality_Assessment_of_Video_(LIVE_VQA):processing-scores}

    \subsection{Objective VQA Algorithms}\label{subsec:Study_of_Subjective_and_Objective_Quality_Assessment_of_Video_(LIVE_VQA):obj-vqa}

    \newpage


    \section{Video (Language) Modeling: A Baseline for Generative Models of Natural Videos}\label{sec:Video_(Language)_Modeling_A_Baseline_for_Generative_Models_of_Natural_Videos}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Marc' Aurelio Ranzato, Michael Mathieu
        \item Institutions: Facebook AI Research
        \item Project website: \url{https://research.fb.com/publications/video-language-modeling-a-baseline-for-generative-models-of-natural-videos/}
        \item Code:
        \item Published in: 2014
        \item Citations: 242 (As of 20-10-2019)
    \end{itemize}

    \subsection*{Abstract}
    \begin{itemize}
        \item Strong baseline for unsupervised feature learning using video data.
        Features are learnt by learning to predict missing frames or extrapolate future frames.
    \end{itemize}

    \subsection{Introduction}\label{subsec:Video_(Language)_Modeling_A_Baseline_for_Generative_Models_of_Natural_Videos:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Model}\label{subsec:Video_(Language)_Modeling_A_Baseline_for_Generative_Models_of_Natural_Videos:model}

    \subsection{Experiments}\label{subsec:Video_(Language)_Modeling_A_Baseline_for_Generative_Models_of_Natural_Videos:experiments}
    \begin{itemize}
        \item Datasets: UCF-101, van Hateren
        \item Baselines:
        \item Evaluation Metrics: RMSE,
    \end{itemize}
    \newpage


    \section{Unsupervised Learning of Video Representations using LSTMs}\label{sec:Unsupervised_Learning_of_Video_Representations_using_LSTMs}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Nitish Srivastava
        \item Institutions: University of Toronto
        \item Project website:
        \item Code: \url{https://github.com/mansimov/unsupervised-videos}
        \item Published in: ICML 2015
        \item Citations: 1109 (As of 19-10-2019)
    \end{itemize}

    \subsection*{Abstract}
    \begin{itemize}
        \item LSTMs to learn representations of video sequences.
        \item Encoder maps input sequence to fixed length representation.
        \item Decoder decodes this to reconstruct the input sequence or predict future sequence.
        \item \textbf{These video representations help improve classification accuracy (eg: action recognition), especially when there are only few training examples.}
    \end{itemize}

    \subsection{Introduction}\label{subsec:Unsupervised_Learning_of_Video_Representations_using_LSTMs:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Model Description}\label{subsec:Unsupervised_Learning_of_Video_Representations_using_LSTMs:model-description}

    \subsubsection{LSTM Autoencoder Model}\label{subsubsec:Unsupervised_Learning_of_Video_Representations_using_LSTMs:lstm-autoencoder-model}

    \subsubsection{LSTM Future Predictor Model}\label{subsubsec:Unsupervised_Learning_of_Video_Representations_using_LSTMs:lstm-future-predictor-model}

    \subsubsection{Conditional Decoder}\label{subsubsec:Unsupervised_Learning_of_Video_Representations_using_LSTMs:conditional-decoder}

    \subsubsection{Composite Model}\label{subsubsec:Unsupervised_Learning_of_Video_Representations_using_LSTMs:composite-model}
    \begin{itemize}
        \item The two tasks: Reconstructing the input and predicting the future are combined in the composite model.
        \item This composite model tries to overcome the shortcomings that each model suffers on its own.
        \item A high-capacity autoencoder would suffer from the tendency to learn trivial representations that just memorize the inputs.
        However, this memorization is not useful at all for predicting the future.
        Therefore, the composite model cannot just memorize information.
        \item On the other hand, the future predictor suffers form the tendency to store information only about the last few frames since those are most important for predicting the future.
        But if we ask the model to also predict all of the input sequence, then it cannot just pay attention to the last few frames.
    \end{itemize}

    \subsection{Experiments}\label{subsec:Unsupervised_Learning_of_Video_Representations_using_LSTMs:experiments}

    \subsubsection{Training}\label{subsubsec:Unsupervised_Learning_of_Video_Representations_using_LSTMs:training}
    \begin{itemize}
        \item Optimization: RMSProp
    \end{itemize}

    \subsubsection{Datasets}\label{subsubsec:Unsupervised_Learning_of_Video_Representations_using_LSTMs:datasets}
    \begin{itemize}
        \item Moving MNIST
        \item UCF-101: 13,320 videos;
        Average length 6.2s;
        101 different action categories;
        9500 videos in train split.
        \item HMDB-51: 5100 videos;
        Average length 3.2s;
        51 different action categories;
        3570 videos in train split.
        \item Sports-1M: Used to train only unsupervised models;
        Resolution: 240x320;
        Center-cropped to 224x224.
    \end{itemize}

    \subsubsection{Visualization and Qualitative Analysis}\label{subsubsec:Unsupervised_Learning_of_Video_Representations_using_LSTMs:visualization-and-qualitative-analysis}
    \begin{itemize}
        \item Moving MNIST: Context: 10 frames;
        Prediction: 10 frames;
        Logistic output units;
        Cross-Entropy Loss;
        \item UCF-101: Conditional Future Predictor;
        32x32 patches;
        ReLU output units;
        Squared Loss;
        Context: 16 frames;
        Prediction: 13 frames;
    \end{itemize}

    \subsubsection{Action Recognition on UCF-101/HMDB-51}\label{subsubsec:Unsupervised_Learning_of_Video_Representations_using_LSTMs:action-recognition}

    \subsubsection{Comparison of Different Model Variants}\label{subsubsec:Unsupervised_Learning_of_Video_Representations_using_LSTMs:comparison-of-different-model-variants}
    \begin{itemize}
        \item Composite Model always does a better job of predicting the future compared to the Future Predictor.
    \end{itemize}
    \begin{itemize}
        \item Baselines:
        \item Evaluation Metrics: Cross-Entropy for MNIST and Squared loss for UCF-101.
    \end{itemize}
    \newpage


    \section{Action-Conditional Video Prediction using Deep Networks in Atari Games}\label{sec:Action_Conditional_Video_Prediction_using_Deep_Networks_in_Atari_Games}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Junhyuk Oh
        \item Institutions: University of Michigan
        \item Project website: \url{https://sites.google.com/a/umich.edu/junhyuk-oh/action-conditional-video-prediction}
        \item Code: \url{https://github.com/junhyukoh/nips2015-action-conditional-video-prediction}
        \item Published in: NIPS 2015
        \item Citations: 434 (As of 22-10-2019)
    \end{itemize}

    \subsection*{Abstract}
    \begin{itemize}
        \item Approximately 100-step action-conditional futures can be predicted.
    \end{itemize}

    \subsection{Introduction}\label{subsec:Action_Conditional_Video_Prediction_using_Deep_Networks_in_Atari_Games:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Action_Conditional_Video_Prediction_using_Deep_Networks_in_Atari_Games:related-work}

    \subsection{Proposed Architectures and Training Method}\label{subsec:Action_Conditional_Video_Prediction_using_Deep_Networks_in_Atari_Games:proposed-architectures}
    \begin{itemize}
        \item Loss Functions: MSE
    \end{itemize}

    \subsection{Experiments}\label{subsec:Action_Conditional_Video_Prediction_using_Deep_Networks_in_Atari_Games:experiments}
    \begin{itemize}
        \item Datasets:
        \item Baselines:
        \item Evaluation Metrics: MSE
    \end{itemize}
    \newpage


    \section{Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting}\label{sec:Convolutional_LSTM_Network_A_Machine_Learning_Approach_for_Precipitation_Nowcasting}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors:
        \item Institutions: Hong Kong University of Science and Technology, China
        \item Project website:
        \item Code:
        \item Published in: NIPS 2015
        \item Citations: 1161 (As of 28-09-2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Convolutional_LSTM_Network_A_Machine_Learning_Approach_for_Precipitation_Nowcasting:introduction}
    \begin{itemize}
        \item \textbf{Highlights}:
        \begin{itemize}
            \item \textbf{One of the first Video Prediction papers}
            \item \textbf{Introduced ConvLSTM}
        \end{itemize}
        \item Motivation: Weather Forecasting
        \item Contributions:
        \begin{itemize}
            \item ConvLSTM
        \end{itemize}
    \end{itemize}

    \subsection{Preliminaries}\label{subsec:Convolutional_LSTM_Network_A_Machine_Learning_Approach_for_Precipitation_Nowcasting:preliminaries}

    \subsection{The Model}\label{subsec:Convolutional_LSTM_Network_A_Machine_Learning_Approach_for_Precipitation_Nowcasting:model}
    \begin{itemize}
        \item Loss Functions:
    \end{itemize}

    \subsection{Experiments}\label{subsec:Convolutional_LSTM_Network_A_Machine_Learning_Approach_for_Precipitation_Nowcasting:experiments}
    \begin{itemize}
        \item Datasets: Moving MNIST, Radar Echo Dataset.
        \item Baselines: ROVER, FC-LSTM
        \item Evaluation Metrics: Rainfall-MSE, Critical Success Index (CSI), False Alarm Rate (FAR), Probability Of Detection (POD), and correlation.
    \end{itemize}
    \newpage


    \section{Sports Videos in the Wild (SVW): A Video Dataset for Sports Analysis}\label{sec:Sports_Videos_in_the_Wild_(SVW)_A_Video_Dataset_for_Sports_Analysis}
    \subsection*{Abstract}
    \begin{itemize}
        \item 4200 videos
        \item Captured in smartphone
        \item Coordinates of a rectangle enclosing action is provided.
    \end{itemize}
    \newpage


    \section{Deep Multi-Scale Video Prediction Beyond Mean Square Error}\label{sec:Deep_Multi_Scale_Video_Prediction_Beyond_Mean_Square_Error}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Michael Mathieu, Camille Couprie, Yann LeCun
        \item Institutions: New York University, Facebook Artificial Intelligence Research
        \item Project website: \url{https://cs.nyu.edu/~mathieu/iclr2016.html}
        \item Code: \url{https://github.com/coupriec/VideoPredictionICLR2016}
        \item Published: ICLR-2016.
        \item Citations: 703 (As of 11-Aug-2019)
    \end{itemize}

    \subsection*{Abstract}
    Pixel-space video prediction
    \begin{itemize}
        \item Multi-scale architecture
        \item Adversarial Training Method
        \item Image Gradient Difference Loss Function
    \end{itemize}

    \subsection{Introduction}\label{subsec:Deep_Multi_Scale_Video_Prediction_Beyond_Mean_Square_Error:introduction}
    \begin{itemize}
        \item Motivation: Unsupervised Learning, Robotics, Video compression, Inpainting
        \item Action recognition algorithm need Supervised Training for months with heavily labelled data.
        This can be reduced by unsupervised learning.
        \item Contributions:
        \begin{itemize}
            \item Adversarial Loss
            \item Image Gradient Difference Loss
            \item Multi-scale architecture
            \item Sharpness measure
        \end{itemize}
    \end{itemize}

    \subsection{Models}\label{subsec:Deep_Multi_Scale_Video_Prediction_Beyond_Mean_Square_Error:models}
    \begin{itemize}
        \item ConvNet: Alternating Convolutions and ReLU layers
        \item Notations:
        \begin{itemize}
            \item X: Sequence of Input Frames.
            \item G(X): Sequence of predicted frames.
            \item Y: Sequence of actual future frames.
        \end{itemize}
        \item $\mathscr{L}_p(X,Y)$ = $l_p(G(X),Y) = \left\Vert G(X) - Y\right\Vert_p^p$
        \item Two major flaws:
        \begin{enumerate}
            \item Convolutions only account for short-range dependencies, limited by the size of their kernels.
            \begin{itemize}
                \item Pooling can only be part of solution: Pooling reduces resolution but we want the output to be of same resolution as input.
                \item Simplest and oldest: No pooling, just many convolution layers.
                \item Skip connections.
                \item Multi-scale architecture using Laplacian Pyramid: This is followed in the paper.
            \end{itemize}
            \item Using $l_2$ loss produces blurry predictions:
            \begin{itemize}
                \item Suppose there are 2 possibilities: $v_1$, $v_2$.
                Using either of them adds $(v_1-v_2)^2$ (Or 0) to loss but using $v_{avg}=(v_1+v_2)/2$ adds a loss of $(v_1-v_2)^2/4$ every time.
                Thus, $v_{avg}$ minimizes $l_2$ loss, but creates blurry predictions.
                \item Using $l_1$ norm minimizes this since $l_1$ norm picks median of the set of equally likely values.
            \end{itemize}
        \end{enumerate}
    \end{itemize}

    \subsubsection{Multi-Scale Network}\label{subsubsec:Deep_Multi_Scale_Video_Prediction_Beyond_Mean_Square_Error:multi-scale-network}
    \begin{itemize}
        \item To tackle problem 1 i.e.\ Convolutions account only for short range dependencies.
        \item $W_G$: Set of trainable parameters.
        \item SGD is used for minimization.
    \end{itemize}

    \subsubsection{Adversarial Training}\label{subsubsec:Deep_Multi_Scale_Video_Prediction_Beyond_Mean_Square_Error:adversarial-training}
    \begin{itemize}
        \item To tackle problem 2 i.e.\ blurry predictions.
        \item Discriminator can easily identify $v_{avg}=(v_1+v_2)/2$ as fake.
        Thus blurry predictions are avoided.
        \item Discriminative model is a multi-scale ConvNet with single scalar output.
        \item SGD is used to train the discriminative model.
        \item Loss functions: \\
        \begin{align*}
            \mathcal{L}_{adv}^D(X,Y) &= \sum_{k=1}^{N_{scales}} L_{\textrm{bce}}(D_k(X_k,Y_k), 1) + L_{\textrm{bce}}(D_k(X_k, G_k(X)), 0) \\
            L_{\textrm{bce}}(Y, \hat{Y}) &= - \sum_i \hat{Y}_i \textrm{log}(Y_i) + (1- \hat{Y}_i) \textrm{log}(1-Y_i) \\
            \mathcal{L}_{adv}^G(X,Y) &= \sum_{k=1}^{N_{scales}} L_{\textrm{bce}}(D_k(X_k, G_k(X_k)), 1) \\
        \end{align*}
    \end{itemize}

    \subsubsection{Image Gradient Difference Loss (GDL)}\label{subsubsec:Deep_Multi_Scale_Video_Prediction_Beyond_Mean_Square_Error:image-gradient-difference-lossgdl}
    \begin{itemize}
        \item To tackle problem 2 i.e.\ blurry predictions.
        \item Penalize the differences of image gradient predictions in the generative loss function. \\
        $\mathcal{L}_{gdl}(X,Y) = \sum_{i,j} \left| \left| X_{i,j} - X_{i-1,j} \right| - \left| Y_{i,j} - Y_{i-1,j} \right| \right|^\alpha + \left| \left| X_{i,j-1} - X_{i,j} \right| - \left| Y_{i,j-1} - Y_{i,j} \right| \right|^\alpha $ \\
        where $\alpha \geq 1$
    \end{itemize}

    \subsubsection{Combining Losses}\label{subsubsec:Deep_Multi_Scale_Video_Prediction_Beyond_Mean_Square_Error:combining-losses}
    \begin{equation*}
        \mathscr{L}(X,Y) = \lambda_{adv} \mathscr{L}_{adv}^G(X,Y) + \lambda_{l_p} \mathscr{L}_p(X,Y) + \lambda_{gdl} \mathscr{L}_{gdl}(X,Y)
    \end{equation*}
    This is used as a loss function for the Generator Network

    \subsection{Experiments}\label{subsec:Deep_Multi_Scale_Video_Prediction_Beyond_Mean_Square_Error:experiments}
    \begin{itemize}
        \item Two models:
        \begin{enumerate}
            \item 4 context frames, 1 predicted frame.
            \item 8 context frames, predicted frames.
        \end{enumerate}
    \end{itemize}

    \subsubsection{Datasets}\label{subsubsec:Deep_Multi_Scale_Video_Prediction_Beyond_Mean_Square_Error:datasets}
    \begin{itemize}
        \item Trained on Sports1M, tested on UCF-101.
    \end{itemize}

    \subsubsection{Network Architecture}\label{subsubsec:Deep_Multi_Scale_Video_Prediction_Beyond_Mean_Square_Error:network-architecture}

    \subsubsection{Quantitative Evaluations}\label{subsubsec:Deep_Multi_Scale_Video_Prediction_Beyond_Mean_Square_Error:quantitative-evaluations}
    \begin{itemize}
        \item PSNR, SSIM and sharpness are calculated between true frame $Y$ and predicted frame $\hat{Y}$.
        \item Evaluation of the accuracy of future frames prediction only takes the moving areas of the images into account.
        \item To extract moving areas, EpicFlow method is used. \\
        \url{http://vision.middlebury.edu/flow/code/flow-code-matlab.zip}
        \item Different quality measures are computed in the areas only in the areas where the optical flow is higher than a fixed threshold.
        \item Interesting fact: Training on $l_2$ gives worst PSNR\@.
    \end{itemize}

    \subsubsection{Comparison to Ranzato et al.}\label{subsubsec:Deep_Multi_Scale_Video_Prediction_Beyond_Mean_Square_Error:comparison-to-ranzato-et-al.}

    \subsection{Conclusion}\label{subsec:Deep_Multi_Scale_Video_Prediction_Beyond_Mean_Square_Error:conclusion}
    \newpage


    \section{Learning Visual Predictive Models of Physics for Playing Billiards}\label{sec:Learning_Visual_Predictive_Models_of_Physics_for_Playing_Billiards}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Paper: \href{https://arxiv.org/abs/1511.07404}{arXiv}
        \item Authors: Pulkit Agrawal, Sergey Levine
        \item Institutions: UC Berkeley
        \item Project website:
        \item Code:
        \item Published in: ICLR 2016
        \item Citations: 156 (As of 24-06-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Learning_Visual_Predictive_Models_of_Physics_for_Playing_Billiards:introduction}
    \begin{itemize}
        \item Visual predictive models of physics: models that can enable the agents to visually anticipate the future states of the world.
        \item Motivation:
        \begin{itemize}
            \item A visual predictive model of physics equips an agent with the ability to generate potential future states of the world in response to an action without actually performing that action.
            \item By running multiple internal simulations to imagine the effects of different actions, the agent can perform planning, choosing the action with the best outcome and executing it in the real world.
        \end{itemize}
        \item Contributions:
        \begin{itemize}
            \item Learning dynamical model of the external world directly from visual inputs.
        \end{itemize}
    \end{itemize}

    \subsection{Previous Work}\label{subsec:Learning_Visual_Predictive_Models_of_Physics_for_Playing_Billiards:previous-work}

    \subsection{Learning Predictive Visual Models}\label{subsec:Learning_Visual_Predictive_Models_of_Physics_for_Playing_Billiards:predictive-visual-models}
    \begin{itemize}
        \item Input to model is 4 past frames and the force exercised on an object.
        Output is the predicted velocity of the object
        \item Loss Functions: Weighted MSE between ground truth and predicted velocities of the object.
        \item \textcolor{red}{How did they generate the training data?}
    \end{itemize}

    \subsection{Model Evaluation}\label{subsec:Learning_Visual_Predictive_Models_of_Physics_for_Playing_Billiards:model-evaluation}
    \begin{itemize}
        \item Datasets:
        \item Baselines:
        \item Evaluation Metrics: Error in the angle and magnitude of the predicted velocities.
        \item Error is high near collisions.
    \end{itemize}

    \subsection{Generating Visual Imaginations}\label{subsec:Learning_Visual_Predictive_Models_of_Physics_for_Playing_Billiards:visual-imaginations}
    \begin{itemize}
        \item Next frame is generating by translating each ball by its predicted velocity.
        The predicted frames are fed back for long term predictions.
    \end{itemize}

    \subsection{Using Predictive Visual Models for Action Planning}\label{subsec:Learning_Visual_Predictive_Models_of_Physics_for_Playing_Billiards:action-planning}
    \begin{itemize}
        \item Task: Compute force to move a ball to desired location.
        Force is computing by sampling different values (CMA-ES method) and checking its output using the internal simulator.
        The optimal force is the one that produces the world state that is closest to the target state.
        Not clear if error is computed in pixel space.
    \end{itemize}

    \subsection{My Summary}\label{subsec:Learning_Visual_Predictive_Models_of_Physics_for_Playing_Billiards:my-summary}
    \begin{itemize}
        \item
    \end{itemize}
    \newpage


    \section{Dynamic Filter Networks}\label{sec:Dynamic_Filter_Networks}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Bert De Brabandere
        \item Institutions: iMinds
        \item Project website:
        \item Code: \url{https://github.com/dbbert/dfn}
        \item Published in: NIPS 2016
        \item Citations: 230 (As of 20-10-2019)
    \end{itemize}

    \subsection*{Abstract}
    \begin{itemize}
        \item In CNNs, after training, filters are fixed.
        In DFN, filters are generated dynamically conditioned on the input.
    \end{itemize}

    \subsection{Introduction}\label{subsec:Dynamic_Filter_Networks:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
        \item Dynamic Filter Module consists of 2 parts: Filter generating network and dynamic filtering layer.
    \end{itemize}

    \subsection{Related Work}\label{subsec:Dynamic_Filter_Networks:related-work}

    \subsection{Dynamic Filter Networks}\label{subsec:Dynamic_Filter_Networks:dfn}
    \begin{itemize}
        \item Loss Functions:
    \end{itemize}

    \subsection{Experiments}\label{subsec:Dynamic_Filter_Networks:experiments}
    \begin{itemize}
        \item Datasets: Moving MNIST, Highway Driving Dataset
        \item Baselines: Some ablation
        \item Evaluation Metrics:
    \end{itemize}
    \newpage


    \section{Improved Techniques for Training GANs}\label{sec:Improved_Techniques_for_Training_GANs}
    \subsection*{Abstract}
    \begin{itemize}
        \item Semi-supervised classification on MNIST, CIFAR-10, SVHN datasets.
    \end{itemize}

    \subsection{Introduction}\label{subsec:Improved_Techniques_for_Training_GANs:introduction}
    \begin{itemize}
        \item Training GANs requires finding a Nash equilibrium of a non-convex game with continuous, highdimensional parameters.
        \item GANs are typically trained using gradient descent techniques that are designed to find a low value of a cost function, rather than to find the Nash equilibrium of a game.
        When used to seek for a Nash equilibrium, these algorithms may fail to converge.
        \item Code: \url{https://github.com/openai/improved-gan}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Improved_Techniques_for_Training_GANs:related-work}

    \subsection{Toward Convergent GAN Training}\label{subsec:Improved_Techniques_for_Training_GANs:toward-convergent-gan-training}
    \begin{itemize}
        \item A Nash equilibrium is a point such that the Discriminator cost function is at a minimum w.r.t Discriminator parameters and Generator cost function is at a minimum w.r.t Generator parameters.
        \item Modification (update) to Discriminator parameters to reduce Discriminator loss function can increase Generator loss function and vice-versa.
        Thus, methods like Gradient Descent may fail to converge (achieve Nash equilibrium) for many games.
        \item Eg: One player minimizing $xy$ w.r.t. $x$ and the other player minimizing $-xy$ w.r.t $y$.
        Nash equilibrium in this case is $x=y=0$, but Gradient Descent fails to achieve it.
    \end{itemize}

    \subsubsection{Feature matching}\label{subsubsec:Improved_Techniques_for_Training_GANs:feature-matching}
    \begin{itemize}
        \item Genertor objective is changed from `\textit{maximizing the output of discriminator}' to `\textit{matching the statistics of real data} i.e.\ match the expected value of features on an intermediate layer of discriminator'.
        \item Let $f(x)$ denote the activations on an intermediate layer of the discriminator.
        New objective for generator is to minimize
        \[\Vert \mathbb{E}_{x \sim p_{\textrm{data}}}[f(x)] - \mathbb{E}_{z \sim p_z(z)}[f(G(z))] \Vert_2^2\]
        \item This prevents a generator from overtraining on the current discriminator.
        Thus addresses the instability of GANs.
        \item No theoretical guarantee, but empirical results support this.
    \end{itemize}

    \subsubsection{Minibatch discrimination}\label{subsubsec:Improved_Techniques_for_Training_GANs:minibatch-discrimination}
    \begin{itemize}
        \item Discriminator uses other examples in mini-batch while classifying an input as real or fake.
        \item Shown to prevent Mode-collapse.
    \end{itemize}

    \subsubsection{Historical Averaging}\label{subsubsec:Improved_Techniques_for_Training_GANs:historical-averaging}

    \subsubsection{One-sided label smoothing}\label{subsubsec:Improved_Techniques_for_Training_GANs:one-sided-label-smoothing}
    \begin{itemize}
        \item Label smoothing: Replace the `0 or 1' targets for the classifier with smoothed values `0.1 or 0.9'.
        \item Replacing positive classification targets with $\alpha$ and negative targets with $\beta$, the optimal discriminator becomes $D(x) = \frac{\alpha p_{\textrm{data}}(x) + \beta p_{\textrm{model}}(x)}{p_{\textrm{data}}(x) + p_{\textrm{model}}(x)}$
        \item We smooth only positive labels to $\alpha$ and set $\beta = 0$
    \end{itemize}

    \subsubsection{Virtual batch normalization}\label{subsubsec:Improved_Techniques_for_Training_GANs:virtual-batch-normalization}
    \begin{itemize}
        \item Each example $x$ is normalized based on the statistics collected on a reference batch of examples that are chosen once and fixed at the start of training, and on $x$ itself.
    \end{itemize}

    \subsection{Assessment of image quality}\label{subsec:Improved_Techniques_for_Training_GANs:assessment-of-image-quality}
    \begin{itemize}
        \item Inception Score:
        \begin{itemize}
            \item Apply the Inception model to every generated image to get the conditional label distribution $p(y|x)$.
            Images that contain meaningful objects should have a conditional label distribution p(yjx) with low entropy.
            \item Then find marginal $p(y) = \int p(y|x=G(z))dz$.
            To have diversity in generated images, $p(y)$ should have high entropy.
            \item Combining these two requirements, Inception Score = $\exp(\mathbb{E}_{x}[\textrm{KL}(p(y|x)||p(y))])$.
            The values are exponentiated for easier comparision.
            \item Inception Score is not a good objective for training, but a good evaluation metric.
            \item Number of samples should be large enough $\sim$ 50K
            \item Inception model used: \url{http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz}
        \end{itemize}
    \end{itemize}

    \subsection{Semi-supervised learning}\label{subsec:Improved_Techniques_for_Training_GANs:semi-supervised-learning}

    \subsection{Experiments}\label{subsec:Improved_Techniques_for_Training_GANs:experiments}

    \subsubsection{MNIST}\label{subsubsec:Improved_Techniques_for_Training_GANs:mnist}

    \subsubsection{CIFAR-10}\label{subsubsec:Improved_Techniques_for_Training_GANs:cifar-10}

    \subsubsection{SVHN}\label{subsubsec:Improved_Techniques_for_Training_GANs:svhn}

    \subsubsection{ImageNet}\label{subsubsec:Improved_Techniques_for_Training_GANs:imagenet}

    \subsection{Conclusion}\label{subsec:Improved_Techniques_for_Training_GANs:conclusion}
    \newpage


    \section{Learning to Poke by Poking: Experiential Learning of Intuitive Physics (NIPS 2016)}\label{sec:Learning_to_Poke_by_Poking_Experiential_Learning_of_Intuitive_Physics_(NIPS_2016)}
    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Learning_to_Poke_by_Poking_Experiential_Learning_of_Intuitive_Physics_(NIPS_2016):introduction}
    \begin{itemize}
        \item Predicting the value of every pixel in the next image is a challenging task.
        \item Moreover, in most cases it is not the precise pixel values that are of interest, but the occurrence of a more abstract event.
        \item For example, predicting that a glass jar will break when pushed from the table onto the ground is of greater interest (and easier) than predicting exactly how every piece of shattered glass will look.
        \item A forward model predicts the next state from the current state and action, and an inverse model predicts the action given the initial and target state.
        \item In joint training, the inverse model objective provides supervision for transforming image pixels into an abstract feature space, which the forward model can then predict.
        \item Baxter Robot is used to poke at objects.
        \item 400 hours of training, 100K pokes, 16 distinct objects.
    \end{itemize}

    \subsection{Data}\label{subsec:Learning_to_Poke_by_Poking_Experiential_Learning_of_Intuitive_Physics_(NIPS_2016):data}
    \begin{itemize}
        \item The robot is equipped with a Kinect camera and a gripper for poking objects kept on a table in front of it.
        \item Only 3 objects at any given time (during training).
        \item Coordinate system:
        \begin{itemize}
            \item X axis: Horizontal axis of robot.
            \item Y axis: Vertical axis of robot.
            \item Z axis: Pointed away from the robot.
        \end{itemize}
        \item Robot moves its finger along XZ plane at fixed height from the table.
        \item Point-cloud data from Kinect depth camera is used to prevent random poking (where object is not present).
        Only during training.
        While testing, RGB data is enough.
        \item Parameters in a poke:
        \begin{itemize}
            \item Point to poke $p$ (random)
            \item Poke direction $\theta$ (random)
            \item Length $l$
            \item Action $u_t$ = ($p$, $\theta$, $l$)
            \item $p_1$: $\frac{l}{2}$ distance from \textit{p} along direction $\theta$
            \item $p_2$: $\frac{l}{2}$ distance from \textit{p} along direction ($\theta$ + 180$^o$)
        \end{itemize}
        Robot executes poke by moving its finger from $p_1$ to $p_2$
        \item Sometimes objects move in unexpected ways.
    \end{itemize}

    \subsection{Method}\label{subsec:Learning_to_Poke_by_Poking_Experiential_Learning_of_Intuitive_Physics_(NIPS_2016):method}
    \begin{itemize}
        \item Notation:
        \begin{itemize}
            \item $I_t$: Image at time $t$
            \item $u_t$: Action at time $t$
            \item $x_t$: Feature (state representation) of image at time $t$
            \item W$_{fwd}$, W$_{inv}$: Forward and Inverse model parameters
            \item F, G: Forward and Inverse models
        \end{itemize}
        \item Prediction in feature space is less challenging than prediction in pixel space.
        \item Feature extracted by a Image Classification Net may not be useful for object manipulation.
        \item Inverse problem formulation prevents degenerate solution of all features reducing to zeros. \textcolor{red}{Didn't understand how}.
        \item \textcolor{red}{Didn't understand second challenge in forward models}.
    \end{itemize}

    \subsubsection{Model}\label{subsubsec:Learning_to_Poke_by_Poking_Experiential_Learning_of_Intuitive_Physics_(NIPS_2016):model}
    \begin{itemize}
        \item Each training sample consists of ($I_t$, $I_{t+1}$, $u_t$)
    \end{itemize}

    \subsubsection{Evaluation Procedure}\label{subsubsec:Learning_to_Poke_by_Poking_Experiential_Learning_of_Intuitive_Physics_(NIPS_2016):evaluation-procedure}
    \begin{itemize}
        \item Greedy approach is followed for repeated iterative pokes.
        Limitation: Can't push around obstacles.
        \item Error Metrics: Location error and Pose error.
    \end{itemize}

    \subsubsection{Blob Model}\label{subsubsec:Learning_to_Poke_by_Poking_Experiential_Learning_of_Intuitive_Physics_(NIPS_2016):blob-model}

    \subsection{Results}\label{subsec:Learning_to_Poke_by_Poking_Experiential_Learning_of_Intuitive_Physics_(NIPS_2016):results}

    \subsection{Related Work}\label{subsec:Learning_to_Poke_by_Poking_Experiential_Learning_of_Intuitive_Physics_(NIPS_2016):related-work}

    \subsection{Discussion and Future Work}\label{subsec:Learning_to_Poke_by_Poking_Experiential_Learning_of_Intuitive_Physics_(NIPS_2016):discussion-and-future-work}

    \subsection{My Summary}\label{subsec:Learning_to_Poke_by_Poking_Experiential_Learning_of_Intuitive_Physics_(NIPS_2016):my-summary}

    \subsection{My Inferences}\label{subsec:Learning_to_Poke_by_Poking_Experiential_Learning_of_Intuitive_Physics_(NIPS_2016):my-inferences}
    \begin{itemize}
        \item Moves to correct location, but not to correct pose.
    \end{itemize}
    \newpage


    \section{Unsupervised Learning for Physical Interaction through Video Prediction}\label{sec:Unsupervised_Learning_for_Physical_Interaction_through_Video_Prediction}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Chelsea Finn, Ian Goodfellow, Sergey Levine
        \item Institutions: UC Berkeley, OpenAI, Google Brain
        \item Project website: \url{https://sites.google.com/site/robotprediction/}
        \item Code: \url{https://github.com/tensorflow/models/tree/master/research/video_prediction}
        \item Published in: NIPS 2016
        \item Citations: 400 (As of 19-10-2019)
        \item Pre-trained Models: Not available.
        Even authors don't have.
    \end{itemize}

    \subsection*{Abstract}
    \begin{itemize}
        \item Learning about physical objects' motion by predicting frames (i.e.\ in pixel space), enables to generalize previously unseen objects.
    \end{itemize}

    \subsection{Introduction}\label{subsec:Unsupervised_Learning_for_Physical_Interaction_through_Video_Prediction:introduction}
    \begin{itemize}
        \item Motivation: Predicting the effect of physical interactions is a critical challenge for learning agents acting in the world, such as robots, autonomous cars, and drones.
        \item Contributions:
        \begin{itemize}
            \item Making long-range predictions in real-world videos by predicting pixel motion.
        \end{itemize}
        \item Three motion prediction modules:
        \begin{enumerate}
            \itemsep0em
            \item DNA: Outputs a distribution over locations in the previous frame for each pixel in the new frame.
            The predicted pixel value is computed as an expectation under this distribution.
            \item CDNA: Outputs the parameters of multiple normalized convolution kernels to apply to the previous image to compute new pixel values.
            The idea is that pixels on the same rigid object will move together, and therefore can share the same transformation.
            \item STP: Outputs the parameters of multiple affine transformations to apply to the previous image.
        \end{enumerate}
        \item Dataset of 59,000 robot pushing motions, consisting of 1.5 million frames and the corresponding actions at each time step.
    \end{itemize}

    \subsection{Related Work}\label{subsec:Unsupervised_Learning_for_Physical_Interaction_through_Video_Prediction:related-work}

    \subsection{Motion-Focused Predictive Models}\label{subsec:Unsupervised_Learning_for_Physical_Interaction_through_Video_Prediction:motion-focused-predictive-models}
    \begin{itemize}
        \item Model computes the next frame by first predicting the motions of image segments, then merges these predictions via masking.
    \end{itemize}

    \subsubsection{Pixel Transformations for Future Video Prediction}\label{subsubsec:Unsupervised_Learning_for_Physical_Interaction_through_Video_Prediction:pixel-transformations-for-future-video-prediction}

    \subsubsection{Composing Object Motion Predictions}\label{subsubsec:Unsupervised_Learning_for_Physical_Interaction_through_Video_Prediction:composing-object-motion-predictions}
    \begin{itemize}
        \item CDNA and STP produce multiple object motion predictions, which need to be combined into a single image.
        \item For each model, including DNA, we also include a "background mask" where we allow the models to copy pixels directly from the previous frame.
        Besides improving performance, this also produces interpretable background masks
    \end{itemize}

    \subsubsection{Action-conditioned Convolutional LSTMs}\label{subsubsec:Unsupervised_Learning_for_Physical_Interaction_through_Video_Prediction:action-conditioned-convolutional-lstms}

    \subsection{Robotic Pushing Dataset}\label{subsec:Unsupervised_Learning_for_Physical_Interaction_through_Video_Prediction:robotic-pushing-dataset}
    \begin{itemize}
        \itemsep0em
        \item Dataset: 10robotic arms;
        57000 interaction sequences;
        1.5 million video frames;
        2 test sets - each with 1250 recorded motions;
        First test set contains 2 subsets of objects seen during training;
        Second test set contains 2 subsets of objects unseen during training. \\
        \url{https://sites.google.com/site/brainrobotdata/home/push-dataset}
    \end{itemize}

    \subsection{Experiments}\label{subsec:Unsupervised_Learning_for_Physical_Interaction_through_Video_Prediction:experiments}
    \begin{itemize}
        \item Datasets: Robotic Pushing Dataset and Human Actions 3.6M dataset.
        \item Baselines: Copying the previous frame, Beyond MSE, Action-conditional video prediction in atari games.
        \item Evaluation Metrics: PSNR and SSIM
    \end{itemize}

    \subsubsection{Action-conditioned prediction for robotic pushing}\label{subsubsec:Unsupervised_Learning_for_Physical_Interaction_through_Video_Prediction:action-conditioned-prediction-for-robotic-pushing}
    \begin{itemize}
        \item Action conditioned prediction
        \item 2 context frames, 8 prediction frames.
        While testing, 18 prediction frames.
    \end{itemize}

    \subsubsection{Human motion prediction}\label{subsubsec:Unsupervised_Learning_for_Physical_Interaction_through_Video_Prediction:human-motion-prediction}
    \begin{itemize}
        \item Videos are subsampled to 10fps.
        \item 10 context frames, 10 prediction frames.
        While testing, 20 prediction frames.
    \end{itemize}

    \subsection{Conclusion and Future Directions}\label{subsec:Unsupervised_Learning_for_Physical_Interaction_through_Video_Prediction:conclusion-and-future-directions}
    \newpage


    \section{Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks}\label{sec:Visual_Dynamics_Probabilistic_Future_Frame_Synthesis_via_Cross_Convolutional_Networks}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Tianfan Xue, Jiajun Wu, William T Freeman
        \item Institutions: Massachusetts Institute of Technology, Google Research
        \item Project website: \url{http://visualdynamics.csail.mit.edu/}
        \item Code: \url{https://github.com/tfxue/visual-dynamics}
        \item Published in: NIPS 2016
        \item Citations: 227 (As of 20-10-2019)
    \end{itemize}

    \subsection*{Abstract}
    \begin{itemize}
        \item In contrast to traditional methods, which have tackled this problem in a deterministic or non-parametric way, we propose a novel approach that models future frames in a probabilistic manner.
    \end{itemize}

    \subsection{Introduction}\label{subsec:Visual_Dynamics_Probabilistic_Future_Frame_Synthesis_via_Cross_Convolutional_Networks:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item Conditional Variational Autoencoder
            \item Intrinsic representation of difference image (Eulerian motion)
            \item Motion modelling using a set of image-dependent convolution kernels operating over an image pyramid.
        \end{itemize}
        \item In this work, we study the problem of visual dynamics: modeling the conditional distribution of future frames given an observed image.
    \end{itemize}

    \subsection{Related Work}\label{subsec:Visual_Dynamics_Probabilistic_Future_Frame_Synthesis_via_Cross_Convolutional_Networks:related-work}

    \subsection{Formulation}\label{subsec:Visual_Dynamics_Probabilistic_Future_Frame_Synthesis_via_Cross_Convolutional_Networks:formulation}
    \begin{itemize}
        \item Loss Functions: MLE, Variational upper bound
    \end{itemize}

    \subsection{Method}\label{subsec:Visual_Dynamics_Probabilistic_Future_Frame_Synthesis_via_Cross_Convolutional_Networks:method}

    \subsection{Evaluations}\label{subsec:Visual_Dynamics_Probabilistic_Future_Frame_Synthesis_via_Cross_Convolutional_Networks:evaluations}
    \begin{itemize}
        \item Datasets: Movement of 2D Shapes, Movement of Video Game Sprites, PENN
        \item Baselines:
        \item Evaluation Metrics: MSE
    \end{itemize}
    \newpage


    \section{Generating Videos with Scene Dynamics}\label{sec:Generating_Videos_with_Scene_Dynamics}
    \subsection*{Abstract}
    \begin{itemize}
        \item Generates tiny videos up to a second at full frame rate.
    \end{itemize}

    \subsection{Introduction}\label{subsec:Generating_Videos_with_Scene_Dynamics:introduction}
    \begin{itemize}
        \item Interested in the fundamental problem of learning how scenes transform with time.
    \end{itemize}
    \newpage


    \section{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DC GAN)}\label{sec:Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative_Adversarial_Networks_(DC_GAN)}

    \subsection{Introduction}\label{subsec:Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative_Adversarial_Networks_(DC_GAN):introduction}
    \begin{itemize}
        \item One way to build good image representations is by training GANs
        \item Later, parts of Generator and Discriminator can be reused as feature extractors for supervised tasks.
        \item GANs provide an alternative to Maximum Likelihood Techniques.
        \item GANs have been known to be unstable to train, often resulting in generators that produce non-sensical outputs.
    \end{itemize}

    \subsection{Related Work}\label{subsec:Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative_Adversarial_Networks_(DC_GAN):related-work}

    \subsection{Approach and Model Architecture}\label{subsec:Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative_Adversarial_Networks_(DC_GAN):model-architecture}
    Proposed changes to CNN architecture:
    \begin{enumerate}
        \item Use all Convolutional Net.
        Replace deterministic spatial pooling functions with strided convolutions.
        It allows network to learn its own spatial downsampling.
        \begin{itemize}
            \item For generator, replace pooling layers by fractional-strided convolutional layers.
            \item For discriminator, replace pooling layers by strided convolutional layers.
        \end{itemize}
        \item Eliminate fully connected layers on top of convolutional features.
        Global average pooling increases model stability, but slows down convergence.
        Use a middle ground between them.
        \begin{itemize}
            \item For generator, connect the noise vector input (noise distribution is uniform) is fully connected to the next layer.
            The result is then reshaped into a 4-dimensional tensor.
            \item For discriminator, the last convolutional layer is flattened and then fed into a single sigmoid output.
        \end{itemize}
        \item Use Batch Normalization.
        \begin{itemize}
            \item Stabilizes learning by normalizing the input to each unit to have zero mean and unit variance.
            \item This prevents generator from collapsing all samples to a single point
            \item Directly applying batchnorm to all layers resulted in sample oscillation and model instability.
            \item This was avoided by not applying batchnorm to the generator output layer and the discriminator input layer.
        \end{itemize}
        \item For generator, use ReLU activation except for output layer.
        Use Tanh activation for output layer. \\
        For discriminator, use Leaky ReLU activation (Original GAN uses maxout activation).
    \end{enumerate}

    \subsection{Details of Adversarial Training}\label{subsec:Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative_Adversarial_Networks_(DC_GAN):details-of-adversarial-training}
    \begin{itemize}
        \item Trained on 3 datasets:
        \begin{enumerate}
            \item LSUN: Large-scale Scene Understanding
            \item Imagenet-1k
            \item Faces dataset
        \end{enumerate}
        \item Preprocessing: Images are scaled to [-1,1] i.e.\ range of tanh activation
        \item Optimizer: SGD with mini-batch-size=128
        \item Weights Initialization $\sim \mathcal{N}(0,0.02)$
        \item LeakyReLU slope = 0.2 in all models
        \item Adam Optimizer: learning rate=0.0002;
        momentum term $\beta_1=0.5$
    \end{itemize}


    \newpage


    \section{Flexible Spatio-Temporal Networks for Video Prediction}\label{sec:Flexible_Spatio_Temporal_Networks_for_Video_Prediction}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Chaochao Lu
        \item Institutions: University of Cambridge, Max Planck Institute for Intelligent Systems
        \item Project website:
        \item Code:
        \item Published in: CVPR 2017
        \item Citations: 33 (As of 22-10-2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Flexible_Spatio_Temporal_Networks_for_Video_Prediction:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item Versatile and flexible framework for video extrapolation and interpolation.
            \item Novel objective function.
            \item Different optimization strategies.
            \item Comprehensive comparison of recent state-of-the-art video prediction methods.
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Flexible_Spatio_Temporal_Networks_for_Video_Prediction:related-work}

    \subsection{Model Description}\label{subsec:Flexible_Spatio_Temporal_Networks_for_Video_Prediction:model-description}
    \begin{itemize}
        \item Loss Functions: $l_2$ loss, Huber loss, DeePSiM loss, adversarial loss
    \end{itemize}

    \subsection{Training}\label{subsec:Flexible_Spatio_Temporal_Networks_for_Video_Prediction:training}

    \subsection{Experiments}\label{subsec:Flexible_Spatio_Temporal_Networks_for_Video_Prediction:experiments}
    \begin{itemize}
        \item Datasets: UCF-101, Sports-1M, PROST, ViSOR, Moving MNIST
        \item Baselines: Spatio-Temporal Video Autoencoder, Beyond MSE, Video (language) modeling baseline
        \item Evaluation Metrics: PSNR, sharpness
    \end{itemize}
    \newpage


    \section{Generating the Future with Adversarial Transformers}\label{sec:Generating_the_Future_with_Adversarial_Transformers}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Carl Vondrick, Antonio Torralba
        \item Institutions: MIT
        \item Project website:
        \item Code:
        \item Published in: CVPR 2017
        \item Citations: 97 (As of 12-04-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Generating_the_Future_with_Adversarial_Transformers:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Generating_the_Future_with_Adversarial_Transformers:related-work}

    \subsection{Dataset}\label{subsec:Generating_the_Future_with_Adversarial_Transformers:dataset}
    \begin{itemize}
        \item Motion stabilized flickr videos
    \end{itemize}

    \subsection{Method}\label{subsec:Generating_the_Future_with_Adversarial_Transformers:method}
    \begin{itemize}
        \item Loss Functions:
        \item 4 past frames, 12 future frames.
    \end{itemize}

    \subsection{Experiments}\label{subsec:Generating_the_Future_with_Adversarial_Transformers:experiments}
    \begin{itemize}
        \item Resolution: $64 \times 64$
        \item Datasets:
        \item Baselines:
        \item Evaluation Metrics: 2AFC experiment
    \end{itemize}
    \newpage


    \section{Self-Supervised Visual Planning with Temporal Skip Connections}\label{sec:Self_Supervised_Visual_Planning_with_Temporal_Skip_Connections}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine
        \item Institutions: UC Berkeley, Technical University of Munich
        \item Project website: \url{https://sites.google.com/view/sna-visual-mpc}
        \item Code:
        \item Published in: CoRL 2017
        \item Citations: 60 (As of 20-10-2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Self_Supervised_Visual_Planning_with_Temporal_Skip_Connections:introduction}
    \begin{itemize}
        \item Motivation: Robotics
        \item Contributions:
        \begin{itemize}
            \item Video Prediction with object occlusions
            \item Long-term planning.
            \item Planning with both discrete and continuous actions with video prediction models.
        \end{itemize}
        \item Suppose in a video, if a moving ball suddenly disappears (when in centre of frame, not edges), then the video doesn't look natural.
        But if there is an occlusion, then it is possible in natural videos as well.
    \end{itemize}

    \subsection{Related Work}\label{subsec:Self_Supervised_Visual_Planning_with_Temporal_Skip_Connections:related-work}

    \subsection{Preliminaries}\label{subsec:Self_Supervised_Visual_Planning_with_Temporal_Skip_Connections:preliminaries}

    \subsection{Skip Connection Neural Advection Model}\label{subsec:Self_Supervised_Visual_Planning_with_Temporal_Skip_Connections:skip-connection-neural-advection-model}

    \subsection{Visual MPC with Pixel Distance Costs}\label{subsec:Self_Supervised_Visual_Planning_with_Temporal_Skip_Connections:visual-mpc-with-pixel-distance-costs}

    \subsection{Sampling-Based MPC with Continuous and Discrete Actions}\label{subsec:Self_Supervised_Visual_Planning_with_Temporal_Skip_Connections:sampling-based-mpc-with-continuous-and-discrete-actions}

    \subsection{Experiments}\label{subsec:Self_Supervised_Visual_Planning_with_Temporal_Skip_Connections:experiments}
    \begin{itemize}
        \item Datasets: BAIR
        \item Baselines: Dynamic Neural Advection (DNA)
        \item Evaluation Metrics:
    \end{itemize}
    \newpage


    \section{Dual Motion GAN for Future-Flow Embedded Video Prediction}\label{sec:Dual_Motion_GAN_for_Future_Flow_Embedded_Video_Prediction}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Xiaodan Liang
        \item Institutions: Carnegie Mellon University, Petuum Inc.
        \item Project website:
        \item Code:
        \item Published in: ICCV 2017
        \item Citations: 93 (As of 21-10-2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Dual_Motion_GAN_for_Future_Flow_Embedded_Video_Prediction:introduction}
    \begin{itemize}
        \item Motivation: Unsupervised video representation learning.
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
        \item Dual Motion GAN learns by explicitly enforcing future-frame predictions to be consistent with the pixel-wise flows in the video through a dual learning mechanism.
        \item The primal future-frame prediction and dual future-flow prediction form a closed loop, generating informative feedback signals to each other for better video prediction.
    \end{itemize}

    \subsection{Related Work}\label{subsec:Dual_Motion_GAN_for_Future_Flow_Embedded_Video_Prediction:related-work}

    \subsection{Dual Motion GAN}\label{subsec:Dual_Motion_GAN_for_Future_Flow_Embedded_Video_Prediction:dual-motion-gan}
    \begin{itemize}
        \item Loss Functions: VAE loss, Adversarial loss
    \end{itemize}

    \subsection{Experiments}\label{subsec:Dual_Motion_GAN_for_Future_Flow_Embedded_Video_Prediction:experiments}
    \begin{itemize}
        \item Datasets: KITTI, Caltech Pedestrian, UCF-101, THUMOS-15
        \item Baselines: Copying previous frame, Beyond MSE, PredNet, DVF, EpicFlow, NextFlow
        \item Evaluation Metrics: MSE, PSNR, SSIM
    \end{itemize}
    \newpage


    \section{Least Squares Generative Adversarial Networks (LS GAN) - ICCV 2017}\label{sec:Least_Squares_Generative_Adversarial_Networks_(LS_GAN)_ICCV_2017}
    \subsection*{Abstract}
    \begin{itemize}
        \item Sigmoid Cross-Entropy loss function causes vanishing gradient problem.
        \item LS GAN uses $\mathcal{L}_2$ distance as loss function.
        \item This is equivalent to minimizing Pearson Divergence
        \item LS GANs generate better quality images compared to normal GANs
        \item LS GANs are more stable during training.
    \end{itemize}

    \subsection{Introduction}\label{subsec:Least_Squares_Generative_Adversarial_Networks_(LS_GAN)_ICCV_2017:introduction}

    \subsection{Related Work}\label{subsec:Least_Squares_Generative_Adversarial_Networks_(LS_GAN)_ICCV_2017:related-work}

    \subsection{Method}\label{subsec:Least_Squares_Generative_Adversarial_Networks_(LS_GAN)_ICCV_2017:method}

    \subsubsection{Generative Adversarial Networks}\label{subsubsec:Least_Squares_Generative_Adversarial_Networks_(LS_GAN)_ICCV_2017:gans}

    \subsubsection{Least Squares Generative Adversarial Networks}\label{subsubsec:Least_Squares_Generative_Adversarial_Networks_(LS_GAN)_ICCV_2017:ls-gans}
    The objective functions for LSGANs is defined as follows:
    \begin{align*}
        \min_D V_{\textrm{LSGAN}}(D) &= \frac{1}{2} \mathbb{E}_{x \sim p_{\textrm{data}}(x)}\left[(D(x) - b)^2\right] +
        \frac{1}{2} \mathbb{E}_{z \sim p_{z}(z)}\left[(D(G(z)) - a)^2\right] \\
        \min_D V_{\textrm{LSGAN}}(G) &= \frac{1}{2} \mathbb{E}_{z \sim p_{z}(z)}\left[(D(G(z)) - c)^2\right]\\
    \end{align*}
    \begin{itemize}
        \item a: Label for fake data
        \item b: Label for real data
        \item c: What Generator wants Discriminator to output for fake data.
    \end{itemize}

    \paragraph{Benefits of LSGANs}

    \paragraph{Relation to Pearson Divergence}

    \paragraph{Parameters Selection}
    \begin{itemize}
        \item $a,b,c$ need to satisfy the following equations: $b-c=1$ and $b-a=2$
        \item Set $a=-1, b=1, c=0$
        \item Another method: Set $c=b$ i.e. $a=0, b=1, c=1$.
        Results in the paper use this.
    \end{itemize}

    \subsection{Experiments}\label{subsec:Least_Squares_Generative_Adversarial_Networks_(LS_GAN)_ICCV_2017:experiments}

    \subsubsection{Datasets and Implementation Details}\label{subsubsec:Least_Squares_Generative_Adversarial_Networks_(LS_GAN)_ICCV_2017:datasets-and-implementation-details}
    \begin{itemize}
        \item Datasets:
        \begin{itemize}
            \item LSUN
            \item CIFAR-10
            \item HWDB1.0
        \end{itemize}
        \item Adam Optimizer: $\beta_1=0.5$
        \item Learning Rate:
        \begin{itemize}
            \item LSUN: 0.001
            \item CIFAR-10: 0.0002
            \item HWDB1.0: 0.0002
        \end{itemize}
        \item Code: \url{https://github.com/xudonmao/LSGAN}
    \end{itemize}

    \subsubsection{Qualitative Evaluation}\label{subsubsec:Least_Squares_Generative_Adversarial_Networks_(LS_GAN)_ICCV_2017:qualitative-evaluation}

    \subsubsection{Quantitative Evaluation}\label{subsubsec:Least_Squares_Generative_Adversarial_Networks_(LS_GAN)_ICCV_2017:quantitative-evaluation}

    \subsubsection{Stability Comparison}\label{subsubsec:Least_Squares_Generative_Adversarial_Networks_(LS_GAN)_ICCV_2017:stability-comparison}

    \paragraph{Suggestions in Practice}
    \begin{itemize}
        \item Sometimes LSGAN suffers from mode collapse at the end of training.
        \item Quality of generated images shift between good and bad during training process.
        \item Based on the above two observations, it is suggested to keep a record of generated images at every thousand or hundred iterations and select the model manually by checking the image quality.
    \end{itemize}

    \subsubsection{Handwritten Chinese Characters}\label{subsubsec:Least_Squares_Generative_Adversarial_Networks_(LS_GAN)_ICCV_2017:handwritten-chinese-characters}

    \subsection{Conclusions and Future Work}\label{subsec:Least_Squares_Generative_Adversarial_Networks_(LS_GAN)_ICCV_2017:conclusions-and-future-work}
    \newpage


    \section{Video Frame Synthesis using Deep Voxel Flow (DVF)}\label{sec:Video_Frame_Synthesis_using_Deep_Voxel_Flow_(DVF)}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Ziwei Liu
        \item Institutions: The Chinese University of Hong Kong, Google
        \item Project website: \url{https://liuziwei7.github.io/projects/VoxelFlow}
        \item Code: \url{https://github.com/liuziwei7/voxel-flow}, \url{https://github.com/lxx1991/pytorch-voxel-flow}
        \item Published in: ICCV 2017
        \item Citations: 169 (As of 20-10-2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Video_Frame_Synthesis_using_Deep_Voxel_Flow_(DVF):introduction}
    \begin{itemize}
        \item Motivation: Film production (Interpolation)
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Video_Frame_Synthesis_using_Deep_Voxel_Flow_(DVF):related-work}

    \subsection{Our Approach}\label{subsec:Video_Frame_Synthesis_using_Deep_Voxel_Flow_(DVF):our-approach}
    \begin{itemize}
        \item Loss Functions:
    \end{itemize}

    \subsection{Experiments}\label{subsec:Video_Frame_Synthesis_using_Deep_Voxel_Flow_(DVF):experiments}
    \begin{itemize}
        \item Datasets: UCF-101, THUMOS-15, KITTI
        \item Baselines: EpicFlow (Interpolation), Beyond MSE (Extrapolation)
        \item Evaluation Metrics: PSNR, SSIM
    \end{itemize}
    \newpage


    \section{Decomposing Motion and Content for Natural Video Sequence Prediction}\label{sec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Ruben Villegas
        \item Institutions: University of Michigan, Adobe Research (San Jose), Google Brain (Mountain View)
        \item Project website: \url{https://sites.google.com/a/umich.edu/rubenevillegas/iclr2017}
        \item Code: \url{https://github.com/rubenvillegas/iclr2017mcnet}
        \item 2017-ICLR conference paper.
        \item Citations: 142 (As of 10-Aug-2019)
    \end{itemize}

    \subsection*{Abstract}
    \begin{itemize}
        \item Decompose motion and content.
        \item Model: Encoder-Decoder ConvNet and ConvLSTM\@.
        \item Datasets: KTH, Weizmann, UCF-101, Sports-1M\@.
    \end{itemize}

    \subsection{Introduction}\label{subsec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction:introduction}
    \begin{itemize}
        \item Motivation: Existing video recognition models can be adopted on top of the predicted frames to infer various semantics of the future.
        \item Predicting high level semantics like action, motion are often specific to the particular task.
        They provide only a partial description of the future.
        Additionally, they require labelled data for training.
        \item Contributions: MCnet, decomposing video into motion and content without separate training (for video prediction task).
    \end{itemize}

    \subsection{Related Work}\label{subsec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction:related-work}

    \subsection{Algorithm Overview}\label{subsec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction:algorithm-overview}
    \begin{itemize}
        \item Objective: Given $x_{1:t}$, predict $\hat{x}_{t+1}$
        \item Motion Encoder: Takes difference of consecutive frames and produces hidden representation $d_t$ that encodes temporal dynamics of the scene components.
        \item Content Encoder: Takes last observed frame $x_t$ and outputs hidden representation $s_t$ that encodes spatial layout of the scene.
        \item Multi-Scale Motion-Content Residual: Takes features from both encoders at every scale before pooling and computes residuals $r_t$
        \item Combination Layers and Decoder: Takes $d_t, s_t, r_t$ and produces $\hat{x}_{t+1}$
        \item Multiple frames can be predicted by recursively feeding the predicted frames as input.
    \end{itemize}

    \subsection{Architecture}\label{subsec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction:architecture}

    \subsubsection{Motion Encoder}\label{subsubsec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction:motion-encoder}
    \begin{itemize}
        \item $[d_t, c_t] = f^{\textrm{dyn}}(x_t - x_{t-1}, d_{t-1}, c_{t-1})$
        \item $c_t$ is a memory cell that retains information of the dynamics observed through time.
        \item $f^{\textrm{dyn}}$ is a fully conv net.
        Encoder CNN with a Convolutional LSTM layer on top.
    \end{itemize}

    \subsubsection{Content Encoder}\label{subsubsec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction:content-encoder}
    \begin{itemize}
        \item Extracts important spatial features from a single frame.
        \item $s_t = f^{\textrm{cont}}(x_t)$
        \item $f^{\textrm{cont}}$ is a CNN\@.
        \item Assymetric architecture for motion and content encoder.
    \end{itemize}

    \subsubsection{Multi-Scale Motion-Content Residual}\label{subsubsec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction:multi-scale-motion-content-residual}
    \begin{itemize}
        \item The residual feature at layer $l$ is computed by $r_t^l = f^{\textrm{res}}([s_t^l, d_t^l])^l$
        \item $[\cdot,\cdot]$ represents concatenation along depth dimension.
        \item $f^{\textrm{res}}(\cdot)^l$ is implemented as consecutive convolution layers and rectification with a final linear layer.
    \end{itemize}

    \subsubsection{Combination Layers and Decoder}\label{subsubsec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction:combination-layers-and-decoder}
    \begin{itemize}
        \item First combines the motion and content back into a unified representation by $f_t = g^{\textrm{comb}}([d_t, d_t])$
        \item $g^{\textrm{comb}}$ is implemented by a CNN with bottleneck layers.
        \item $x_{t+1} = g^{\textrm{dec}}(f_t, r_t)$
        \item $g^{\textrm{dec}}$ is a deconvolution network.
        The output layer is passed through a $\tanh$ activation function.
    \end{itemize}

    \subsection{Inference and Training}\label{subsec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction:inference-and-training}

    \subsubsection{Multi-Step Prediction}\label{subsubsec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction:multi-step-prediction}

    \subsubsection{Training Objective}\label{subsubsec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction:training-objective}
    \begin{itemize}
        \item Loss:
        \begin{align*}
            \mathcal{L} &= \alpha \mathcal{L}_{\textrm{img}} + \beta \mathcal{L}_{\textrm{GAN}} \\
            \mathcal{L}_{\textrm{img}} &= \mathcal{L}_p(x_{t+k}, \hat{x}_{t+k}) + \mathcal{L}_{gdl}(x_{t+k}, \hat{x}_{t+k}) \\
            \mathcal{L}_p(y,z) &= \sum_{k=1}^{T} \Vert y-z \Vert_p^p \\
            \mathcal{L}_{gdl}(y,z) &= \sum_{i,j}^{h,w} \vert (\vert y_{i,j} - y_{i-1,j} \vert - \vert z_{i,j} - z_{i-1,j}\vert) \vert^\lambda + \vert (\vert y_{i,j-1} - y_{i,j} \vert - \vert z_{i,j-1} - z_{i,j}\vert ) \vert^\lambda \\
            \mathcal{L}_{\textrm{GAN}} &= - \textrm{log} D([x_{1:t}, G(x_{1:t})]) \\
            G(x_{1:t}) &= \hat{x}_{t+1:t+T} \\
            \mathcal{L}_{\textrm{disc}} &= - \textrm{log} D([x_{1:t}, x_{t+1:t+T}]) - \textrm{log}(1 - D([x_{1:t}, G(x_{1:t})])) \\
        \end{align*}
    \end{itemize}

    \subsection{Experiments}\label{subsec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction:experiments}
    \begin{itemize}
        \item Evaluated on datasets: KTH, Weizmann action, UCF-101
        \item Baselines: ConvLSTM, Deep Multi-Scale Video Prediction Beyond Mean Square Error (Mathieu, Yann LeCun)
        \item Hyper-parameters: $\alpha=1, \lambda=1, p=2$
    \end{itemize}

    \subsubsection{KTH and Weizmann Action Datasets}\label{subsubsec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction:kth-and-weizmann-action-datasets}
    \begin{itemize}
        \item KTH split: train(1-16), test(17-25).
        \item Frames resized to 128x128
        \item Hyper-parameters: $\beta = 0.02$ for training.
        \item Evaluation: PSNR, SSIM
    \end{itemize}

    \subsubsection{UCF-101 Dataset}\label{subsubsec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction:ucf-101-dataset}
    \begin{itemize}
        \item Resized frames to 240x320.
        \item Observes 4 frames and predicts next frame.
        \item Hyper-parameters: $\beta = 0.001$ for training.
        \item Trained on Sports-1M and tested on UCF-101.
    \end{itemize}

    \subsection{Conclusion}\label{subsec:Decomposing_Motion_and_Content_for_Natural_Video_Sequence_Prediction:conclusion}

    \newpage


    \section{Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning}\label{sec:Deep_Predictive_Coding_Networks_for_Video_Prediction_and_Unsupervised_Learning}
    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Deep_Predictive_Coding_Networks_for_Video_Prediction_and_Unsupervised_Learning:introduction}
    \begin{itemize}
        \item Computer vision models are typically trained using static images.
        But in real visual world, there is movement in both viewer and objects.
        \item Many have suggested that temporal experience with objects as they move and undergo transformations can serve as an important signal for learning about the structure of objects.
        \item PredNet: A deep, recurrent convolutional neural network to continually predict the appearance of future video frames
        \item Inspired from the concept of "Predictive Coding" in Neuroscience.Predictive coding posits that the brain is continually making predictions of incoming sensory stimuli.
    \end{itemize}

    \subsection{The PredNet Model}\label{subsec:Deep_Predictive_Coding_Networks_for_Video_Prediction_and_Unsupervised_Learning:the-prednet-model}

    \subsection{Experiments}\label{subsec:Deep_Predictive_Coding_Networks_for_Video_Prediction_and_Unsupervised_Learning:experiments}

    \subsubsection{Rendered Image Sequences}\label{subsubsec:Deep_Predictive_Coding_Networks_for_Video_Prediction_and_Unsupervised_Learning:rendered-image-sequences}
    \begin{itemize}
        \item Quantitative evaluation of generative models is a difficult, unsolved problem.
        Here, MSE and SSIM are used.
        \item The rotating faces were generated using the FaceGen software package.
        \item To understand the information contained in the trained models, we decoded the latent parameters from the representation neurons ($R_l$) in different layers, using a ridge regression.
    \end{itemize}

    \subsubsection{Natural Image Sequences}\label{subsubsec:Deep_Predictive_Coding_Networks_for_Video_Prediction_and_Unsupervised_Learning:natural-image-sequences}
    \begin{itemize}
        \item Car-mounted camera videos are chosen since these videos span across a wide range of settings and are characterized by rich temporal dynamics, including both self-motion of the vehicle and the motion of other objects in the scene.
        \item Models were trained using the raw videos from the KITTI dataset.
        Tested on the CalTech Pedestrian dataset.
        \item Training dataset: 128x160 resolution 41K frames.
        \item Though trained for 1 frame prediction, can be used to predict upto 9 frames by feeding back the predicted frames.
        Approx 2fps.
        So, given a frame, 5s video is generated.
    \end{itemize}

    \subsection{Discussion}\label{subsec:Deep_Predictive_Coding_Networks_for_Video_Prediction_and_Unsupervised_Learning:discussion}

    \subsection{Appendix}\label{subsec:Deep_Predictive_Coding_Networks_for_Video_Prediction_and_Unsupervised_Learning:appendix}

    \newpage


    \section{Learning to Generate Long-term Future via Hierarchical Prediction}\label{sec:Learning_to_Generate_Long_term_Future_via_Hierarchical_Prediction}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Ruben Villegas
        \item Institutions: Google Brain, University of Michigan, Adobe Research
        \item Project website: \url{https://sites.google.com/a/umich.edu/rubenevillegas/hierch_vid}
        \item Code: \url{https://github.com/rubenvillegas/icml2017hierchvid}
        \item Published in: ICML 2017
        \item Citations: 130 (As of 22-10-2019)
    \end{itemize}

    \subsection*{Abstract}
    \begin{itemize}
        \item To avoid inherent compounding errors in recursive pixel level prediction, we propose to first estimate high level structure in the input frames, then predict how that structure evolves in the future, and finally by observing a single frame from the past and the predicted high-level structure, we construct the future frames without having to observe any of the pixel-level predictions.
    \end{itemize}

    \subsection{Introduction}\label{subsec:Learning_to_Generate_Long_term_Future_via_Hierarchical_Prediction:introduction}
    \begin{itemize}
        \item Motivation: Robotics, Autonomous cars.
        \item Contributions:
        \begin{itemize}
            \item Hierarchical approach for video prediction
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Learning_to_Generate_Long_term_Future_via_Hierarchical_Prediction:related-work}

    \subsection{Overview}\label{subsec:Learning_to_Generate_Long_term_Future_via_Hierarchical_Prediction:overview}

    \subsection{Architecture}\label{subsec:Learning_to_Generate_Long_term_Future_via_Hierarchical_Prediction:architecture}

    \subsection{Training}\label{subsec:Learning_to_Generate_Long_term_Future_via_Hierarchical_Prediction:training}
    \begin{itemize}
        \item Loss Functions: MSE, MSE in feature space (AlexNet and Hourglass Network), adversarial loss, MSE between poses.
    \end{itemize}

    \subsection{Experiments}\label{subsec:Learning_to_Generate_Long_term_Future_via_Hierarchical_Prediction:experiments}
    \begin{itemize}
        \item Datasets: Human 3.6M, Penn Action (10 context, 32 predictions, 64 predictions for testing)
        \item Baselines:
        \item Evaluation Metrics: 2AFC (using AMT), PSNR
    \end{itemize}
    \newpage


    \section{Video Pixel Network (VPN)}\label{sec:Video_Pixel_Network_(VPN)}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Nal Kalchbrenner
        \item Institutions: Google Deep Mind
        \item Project website:
        \item Code:
        \item Published in: ICML 2017
        \item Citations: 177 (As of 20-10-2019)
    \end{itemize}

    \subsection*{Abstract}
    \begin{itemize}
        \item VPN estimates discrete joint distribution of the raw pixel values in a video.
    \end{itemize}

    \subsection{Introduction}\label{subsec:Video_Pixel_Network_(VPN):introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Method}\label{subsec:Video_Pixel_Network_(VPN):method}

    \subsection{Architecture}\label{subsec:Video_Pixel_Network_(VPN):architecture}

    \subsection{Network Building Blocks}\label{subsec:Video_Pixel_Network_(VPN):network-building-blocks}
    \subsection*{Experiments}
    \begin{itemize}
        \item Datasets: Moving MNIST, PUSH
        \item Baselines:
        \item Evaluation Metrics:
    \end{itemize}
    \newpage


    \section{Learning to See Physics via Visual De-animation}\label{sec:Learning_to_See_Physics_via_Visual_De_animation}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Paper: \href{http://papers.nips.cc/paper/6620-learning-to-see-physics-via-visual-de-animation}{NIPS}
        \item Authors: Jiajin Wu, Pushmeet Kohli, William Freeman, Joshua Tenenbaum
        \item Institutions: MIT, Deep Mind
        \item Project website:
        \item Code:
        \item Published in: NIPS 2017
        \item Citations: 100 (As of 25-06-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Learning_to_See_Physics_via_Visual_De_animation:introduction}
    \begin{itemize}
        \item Motivation: Humans can predict what happens next, infer physical properties from visual input and make an unstable system, stable by applying forces.
        \item Contributions:
        \begin{itemize}
            \item Generative pipeline for physical scene understanding.
            \item Learning of physical scene representations without human supervision.
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Learning_to_See_Physics_via_Visual_De_animation:related-work}

    \subsection{Visual De-animation}\label{subsec:Learning_to_See_Physics_via_Visual_De_animation:visual-de-animation}
    \begin{itemize}
        \item Components:
        \begin{itemize}
            \item Perception Module: Does inverse physics i.e.\ generates object representations and estimates their physical state.
            \item Physics Engine: Can be neural network based differentiable engine or classical non-differentiable engines.
            REINFORCE is used for non-differentiable engines.
            \item Graphics Engine: Renders pixel images from object representations.
        \end{itemize}
    \end{itemize}

    \subsection{Evaluation}\label{subsec:Learning_to_See_Physics_via_Visual_De_animation:evaluation}
    \begin{itemize}
        \item Framework is evaluated in 3 scenarios.
        \begin{itemize}
            \item Synthetic Billard Tables
            \item Real world Billiards videos
            \item Block Towers
        \end{itemize}
    \end{itemize}

    \subsubsection{Billiard Tables: A Motivating Example}\label{subsubsec:Learning_to_See_Physics_via_Visual_De_animation:synthetic-billiards}
    \begin{itemize}
        \item 3 types:
        \begin{itemize}
            \item same appearance, same physics
            \item different appearance, different physics
            \item same appearance, different physics
        \end{itemize}
        \item Physical Properties:
        \begin{itemize}
            \item Intrinsic properties:
            \begin{itemize}
                \item Mass
                \item Friction
            \end{itemize}
            \item Extrinsic Properties:
            \begin{itemize}
                \item 2D position
                \item Velocity
            \end{itemize}
        \end{itemize}
        \item Flow fields are computed using SPyNet
        \item Perceptual model is a ResNet-18 which takes masked 3 RGB frames and 2 flow images and recovers object's physical state.
        \item A differential neural physics engine predicts objects' extrinsic properties in the next frame.
        \item Graphics engine renders final images.
        \item Loss functions:
        \begin{itemize}
            \item MSE in pixel space for reconstruction.
            \item From rendered images, objects positions are computed and error in positions is used.
        \end{itemize}
        \item Training:
        \begin{itemize}
            \item Perceptual module and neural physics engine are pretrained on synthetic data (supervised).
            \item End-to-end fine tuning without annotations.
        \end{itemize}
        \item Quantitative measures:
        \begin{itemize}
            \item MSE in pixel space for reconstruction
            \item Manhattan distance for position and velocity prediction
            \item Behavioral study: Subjects are asked to predict future trajectory.
            Ratio of errors in trajectories predicted by model and humans is computed.
        \end{itemize}
    \end{itemize}

    \subsubsection{Billiard Tables: Transferring to Real Videos}\label{subsubsec:Learning_to_See_Physics_via_Visual_De_animation:real-billiards}
    \begin{itemize}
        \item Same as with synthetic billiards, but perception module takes flow images to abstract away appearance changes.
        \item Perception module is retrained.
        \item Neural physics engine trained on synthetic billiards is used.
        \item Predicted frames are rendered using Blender by providing the object states
        \item No quantitative evaluation.
    \end{itemize}

    \subsubsection{The Blocks World}\label{subsubsec:Learning_to_See_Physics_via_Visual_De_animation:block-towers}
    \begin{itemize}
        \item Focus is on reasoning of object states in 3D world rather than physical properties like mass.
        \item All objects have same physical properties (mass, friction etc), just differ in color.
        \item Physical state is the pose (3D position and 3D rotation euler angles)
        \item Perception module is ResNet-18 which takes silhouettes of blocks and recovers physical state.
        \item Bullet physics engine is used.
        \item Blender is used to render predicted frames.
        \item Input is a static image.
        \item Loss functions:
        \begin{itemize}
            \item MSE between rendered silhouettes and observations.
            \item Binary Cross-Entropy between predicted and ground truth stability.
        \end{itemize}
        \item Training:
        \begin{itemize}
            \item Perception module is trained on synthetic data (supervised).
            \item End-to-end finetuning with only ground truth values for stability.
        \end{itemize}
        \item 3 tasks:
        \begin{itemize}
            \item Scene reconstruction
            \item Stability prediction (Future prediction)
            \item Making an unstable tower stable
        \end{itemize}
        \item Evaluation: Accuracy of stability prediction.
        \item Generalization: trained on towers with 2/4 blocks and tested on towers with 3 blocks.
    \end{itemize}

    \subsection{Discussion}\label{subsec:Learning_to_See_Physics_via_Visual_De_animation:discussion}

    \subsection{My Summary}\label{subsec:Learning_to_See_Physics_via_Visual_De_animation:my-summary}
    \begin{itemize}
        \item Main contribution is the framework integrating perception module, physics engine and rendering engine.
        \item Can be used for either estimating physical properties or for video prediction.
        \item Drawbacks:
        \begin{itemize}
            \item Doesn't evaluate the perception module on ground truth for synthetic data.
            \item May not generalize to unseen objects.
            \item There is a scale factor ambiguity in mass and friction coefficient estimated for billiards case - it appears impossible to find out the exact mass and friction coefficient in the current setup.
            \item However, if an external force is applied using a robotic arm or using gravity, then mass can be accurately estimated.
            \item Not clear what use is estimating mass and friction coefficient which is correct only upto a scale factor.
            \item Doesn't fine tune the end-to-end model for real world Billiards case.
            \item MSE may not be an appropriate loss function in fine-tuning for real world Billiards.
            SSIM structure term might be better.
            \item Stability prediction is not an exhaustive evaluation measure for perception task.
            The perception module might have estimated the pose wrongly and still be able to predict the stability correctly.
            The binary score sweeps a lot of factors under the carpet.
            \item Since improvement in accuracy is not significant, it raises the question of effectiveness of this framework.
        \end{itemize}
    \end{itemize}
    \newpage


    \section{Transformation-based Models of Video Sequences}\label{sec:Transformation_based_Models_of_Video_Sequences}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Marc' Aurelio Ranzato, Soumith Chintala
        \item Institutions: Facebook AI Research
        \item Project website:
        \item Code:
        \item Published in: ICLR 2017 Reject
        \item Citations: 32 (As of 12-04-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Transformation_based_Models_of_Video_Sequences:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
        \item Transformation based approach: uses affine transformations.
        \item New evaluation method
    \end{itemize}

    \subsubsection{Related Work}\label{subsubsec:Transformation_based_Models_of_Video_Sequences:related-work}

    \subsection{Model}\label{subsec:Transformation_based_Models_of_Video_Sequences:model}
    \begin{itemize}
        \item Loss Functions:
        \item 4 past frames, 8 predicted frames.
    \end{itemize}

    \subsection{Experiments}\label{subsec:Transformation_based_Models_of_Video_Sequences:experiments}
    \begin{itemize}
        \item Datasets: Moving MNIST, UCF-101
        \item Baselines:
        \item Evaluation Metrics: Classification accuracy for action recognition.
    \end{itemize}

    \subsection{Conclusions}\label{subsec:Transformation_based_Models_of_Video_Sequences:conclusions}
    \newpage


    \section{Deep Image Prior}\label{sec:Deep_Image_Prior}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky
        \item Institutions: Skolkovo Institute of Science and Technology, University of Oxford
        \item Project website: \url{https://dmitryulyanov.github.io/deep_image_prior}
        \item Code: \url{https://github.com/DmitryUlyanov/deep-image-prior}
        \item Published in: CVPR 2018
        \item Citations: 353 (As of 07-01-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Deep_Image_Prior:introduction}

    \subsection{Method}\label{subsec:Deep_Image_Prior:method}

    \subsection{Applications}\label{subsec:Deep_Image_Prior:applications}
    \begin{itemize}
        \item Blind Image Denoising
        \item Super Resolution
        \item Inpainting
        \item Natural Pre Image
        \begin{itemize}
            \item It is a diagnostic tool to study the invariances of a lossy function, such as deep network.
            \item Let $\phi$ be the first several layers of a neural network trained to perform, say, image classification.
            The pre-image is the set of images, that result in same representation $\phi(x_0)$
            \[\phi^{-1}(\phi(x_0)) = \{x \in \mathcal{X} : \phi(x) = \phi(x_0)\}\]
            \item Pre-images can be found by minimizing
            \[ E(x;x_0) = \Vert \phi(x) - \phi(x_0) \Vert ^2\]
            \item Optimizing this function directly may lead to non-natural images.
            By restricting pre-image to set $\mathcal{X}$ of natural images, gives natural pre image.
        \end{itemize}
        \item Flash - No flash reconstruction
    \end{itemize}

    \subsection{Related Work}\label{subsec:Deep_Image_Prior:related-work}

    \subsection{Discussion}\label{subsec:Deep_Image_Prior:discussion}

    \subsection{My Summary}\label{subsec:Deep_Image_Prior:my-summary}
    \begin{itemize}
        \item Use a randomly initialized conditional generator network.
        \item Update network parameters (weights) to minimize loss function, for the given test image.
        \item Overtraining doesn't give good results, but optimal number of iterations at optimal learning rate, gives good images.
    \end{itemize}
    \newpage


    \section{Future Frame Prediction for Anomaly Detection - A New Baseline}\label{sec:Future_Frame_Prediction_for_Anomaly_Detection_A_New_Baseline}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Wen Liu, Weixin Luo, Dongze Lian, Shenghua Gao
        \item Institutions: ShanghaiTech University
        \item Project website:
        \item Code: \url{https://github.com/StevenLiuWen/ano_pred_cvpr2018}
        \item Published in: CVPR 2018
        \item Citations: 90 (As of 12-04-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Future_Frame_Prediction_for_Anomaly_Detection_A_New_Baseline:introduction}
    \begin{itemize}
        \item Motivation:
        \begin{itemize}
            \item Video Prediction can better capture anomalies than video reconstruction.
        \end{itemize}
        \item Apply temporal constraint by minimizing error in optical flow (optical flow is not predicted).
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Future_Frame_Prediction_for_Anomaly_Detection_A_New_Baseline:related-work}

    \subsection{Method}\label{subsec:Future_Frame_Prediction_for_Anomaly_Detection_A_New_Baseline:method}
    \begin{itemize}
        \item Unet based architecture
        \item Loss Functions:
        \begin{itemize}
            \item MSE in pixel space
            \item Gradient difference loss
            \item Flow $l_1$ error
            \item Adversarial loss
        \end{itemize}
        \item 4 past frames, 1 future frame
    \end{itemize}

    \subsection{Experiments}\label{subsec:Future_Frame_Prediction_for_Anomaly_Detection_A_New_Baseline:experiments}
    \begin{itemize}
        \item Datasets: CUHK Avenue, UCSD Pedestrian, ShanghaiTech (Anomaly detection datasets)
        \item Baselines: BeyondMSE
        \item Evaluation Metrics: Area under the curve
    \end{itemize}
    \newpage


    \section{Structure Preserving Video Prediction}\label{sec:Structure_Preserving_Video_Prediction}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Jingwei Xu
        \item Institutions: Shanghai Institute for Advanced Communication and Data Science
        \item Project website:
        \item Code:
        \item Published in: CVPR 2018
        \item Citations: 18 (As of 09-04-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Structure_Preserving_Video_Prediction:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Structure_Preserving_Video_Prediction:related-work}

    \subsection{Methods}\label{subsec:Structure_Preserving_Video_Prediction:methods}
    \begin{itemize}
        \item Loss Functions:
    \end{itemize}

    \subsection{Experiments}\label{subsec:Structure_Preserving_Video_Prediction:experiments}
    \begin{itemize}
        \item Datasets: UCF-101, Human 3.6M, CityScapes
        \item Baselines:
        \item Evaluation Metrics: PSNR, SSIM
        \item 10 past frames, 10 future frames.
    \end{itemize}
    \newpage


    \section{The Unreasonable Effectiveness of Deep Features as a Perceptual Metric}\label{sec:The_Unreasonable_Effectiveness_of_Deep_Features_as_a_Perceptual_Metric}
    \subsection*{Abstract}
    \begin{itemize}
        \item Features extracted by VGG network (pretrained on ImageNet-1k dataset) have been remarkably useful as a training loss for image synethesis.
        But how ``perceptual" are these ``perceptual losses"?
        \item A new dataset of human perceptual similarity judgements is introduced.
        \item Deep features (not restricted to ImageNet trained VGG, all levels of supervision i.e.\ supervised, self-supervised and unsupervised) perform better than PSNR and SSIM in perceptual quality.
        \item Models and data: \url{https://www.github.com/richzhang/PerceptualSimilarity}
    \end{itemize}

    \subsection{Motivation}\label{subsec:The_Unreasonable_Effectiveness_of_Deep_Features_as_a_Perceptual_Metric:motivation}
    \begin{itemize}
        \item What we would really like is a ``perceptual distance", which measures how similar are two images in a way that coincides with human judgment.
        \item Human judgments of similarity
        \begin{itemize}
            \item depend on high-order image structure
            \item are context-dependent
            \item may not actually constitute a distance metric
        \end{itemize}
        \item Deep features perform far better than SSIM and FSIM when there is spatial ambiguity
        \item Randomly initialized networks do not achieve good performance.
        \item Our study is based on a newly collected perceptual similarity dataset, using a large set of distortions and real algorithm outputs.
        It contains both traditional distortions, such as contrast and saturation adjustments, noise patterns, filtering, and spatial warping operations, and CNN-based algorithm outputs, such as autoencoding, denoising, and colorization, produced by a variety of architectures and losses.
        \item Contributions:
        \begin{itemize}
            \item Dataset: \textbf{484k} human judgements.
            \item Deep features model low level perceptual similarity
            \item Network architecture alone is insufficient.
            Training is important.
            \item Performance can be improved by ``calibrating" feature responses from a pre-trained network using the dataset.
        \end{itemize}
    \end{itemize}

    \subsection{Berkeley-Adobe Perceptual Patch Similarity (BAPPS) Dataset}\label{subsec:The_Unreasonable_Effectiveness_of_Deep_Features_as_a_Perceptual_Metric:bappsdataset}
    \begin{itemize}
        \item We collect a large-scale highly diverse dataset of perceptual judgments using two approaches.
        \begin{itemize}
            \item Mainly Two Alternative Forced Choice (\textbf{2AFC}).
            \item Validated by Just Noticeable Difference (\textbf{JND}).
        \end{itemize}
    \end{itemize}

    \subsection{Deep Feature Spaces}\label{subsec:The_Unreasonable_Effectiveness_of_Deep_Features_as_a_Perceptual_Metric:deep-feature-spaces}
    \begin{itemize}
        \item
    \end{itemize}
    \newpage


    \section{ContextVP: Fully Context-Aware Video Prediction}\label{sec:ContextVP_Fully_Context_Aware_Video_Prediction}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Wonmin Byeon
        \item Institutions: NVIDIA
        \item Project website: \url{https://wonmin-byeon.github.io/publication/2018-eccv}
        \item Code:
        \item Published in: ECCV 2018
        \item Citations: 23 (As of 21-10-2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:ContextVP_Fully_Context_Aware_Video_Prediction:introduction}
    \begin{itemize}
        \item Motivation: Unsupervised learning of features.
        \item Contributions:
        \begin{itemize}
            \item Highlighting a blind spot problem of uncertain future leading to blurry predictions.
            \item Simple baseline.
            \item New architecture for video prediction.
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:ContextVP_Fully_Context_Aware_Video_Prediction:related-work}

    \subsection{Missing Contexts in Other Network Architectures}\label{subsec:ContextVP_Fully_Context_Aware_Video_Prediction:missing-contexts-in-other-network-architectures}

    \subsection{Method}\label{subsec:ContextVP_Fully_Context_Aware_Video_Prediction:method}
    \begin{itemize}
        \item Loss Functions: $\mathcal{L}_p$ Loss, Image Gradient Difference Loss
    \end{itemize}

    \subsection{Experiments}\label{subsec:ContextVP_Fully_Context_Aware_Video_Prediction:experiments}
    \begin{itemize}
        \item Datasets:
        \begin{itemize}
            \item Human 3.6M: 10 past frames.
            \item KITTI (train) + CalTech Pedestrian (test): 10 past frames.
            \item UCF101: 4 past frames used.
        \end{itemize}
        \item Baselines: Copying previous frame, ConvLSTM20, Beyond MSE, PredNet, DVF, Dual Motion GAN
        \item Evaluation metrics: PSNR, SSIM
    \end{itemize}
    \newpage


    \section{DYAN: A Dynamical Atoms-Based Network For Video Prediction}\label{sec:DYAN_A_Dynamical_Atoms_Based_Network_For_Video_Prediction}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Wenqian Liu, Abhishek Sharma
        \item Institutions:Northeastern University, Boston
        \item Project website:
        \item Code: \url{https://github.com/liuem607/DYAN}
        \item Published in: ECCV 2018
        \item Citations: 2 (As of 25-09-2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:DYAN_A_Dynamical_Atoms_Based_Network_For_Video_Prediction:introduction}
    \begin{itemize}
        \item Motivation: Autonomous Driving
        \item Contributions:
        \begin{itemize}
            \item A novel auto-encoder network that captures long and short term temporal information and explicitly incorporates dynamics-based affine invariants;
            \item The proposed network is shallow, with very few parameters.
            It is easy to train and it does not take large disk space to save the learned model.
            \item The proposed network is easy to interpret and it is easy to visualize what it learns, since the parameters of the network have a clear physical meaning.
            \item The proposed network can predict future frames accurately and efficiently without introducing blurriness.
            \item The model is differentiable, so it can be fine-tuned for another task if necessary.
        \end{itemize}
        \item RNNs are hard to train for long term predictions because of vanishing and exploding gradients.
        \item LSTMs and GRUs are easier to use.
        \item GANs are hard to reportedly train, since training requires finding a Nash equilibrium of a game, which might be hard to get using gradient descent techniques.
        \item DYAN is similar to LSTMs.
    \end{itemize}

    \subsection{Related Work}\label{subsec:DYAN_A_Dynamical_Atoms_Based_Network_For_Video_Prediction:related-work}
    \begin{itemize}
        \item Most Optical Flow methods focus on Lagrangian Optical Flow: Flow field represents the displacement between corresponding pixels or features across frames.
        \item In Eulerian optical flow, where the motion is captured by the changes at individual pixels, without requiring finding correspondences or tracking features.
        \item Eulerian flow has been shown to be useful for tasks such as motion enhancement and video frame interpolation.
    \end{itemize}

    \subsection{Background}\label{subsec:DYAN_A_Dynamical_Atoms_Based_Network_For_Video_Prediction:background}
    \begin{itemize}
        \item Loss Functions: Sparse optimization, $l_2$ loss.
    \end{itemize}

    \subsection{DYAN: A dynamical atoms-based network}\label{subsec:DYAN_A_Dynamical_Atoms_Based_Network_For_Video_Prediction:dyan}

    \subsection{Implementation Details}\label{subsec:DYAN_A_Dynamical_Atoms_Based_Network_For_Video_Prediction:implementation}

    \subsection{Experiments}\label{subsec:DYAN_A_Dynamical_Atoms_Based_Network_For_Video_Prediction:experiments}
    \begin{itemize}
        \item Datasets: KITTI/Caltech and UCF-101.
        \item Baselines: CopyLastFrame, DualMoGAN, BeyondMSE, PredNet and DVF\@.
        \item Evaluation Metrics: MSE and SSIM\@.
    \end{itemize}

    \subsubsection{Car Mounted Camera Videos Dataset}\label{subsubsec:DYAN_A_Dynamical_Atoms_Based_Network_For_Video_Prediction:kitti-dataset}
    \begin{itemize}
        \item Based on 10 past frames, predicts the next 1 frame.
        \item Videos center-cropped and resized to 128x160.
        \item Baselines: CopyLastFrame, DualMoGAN, BeyondMSE and PredNet.
    \end{itemize}

    \subsubsection{Human Action Videos Dataset}\label{subsubsec:DYAN_A_Dynamical_Atoms_Based_Network_For_Video_Prediction:ucf-dataset}
    \begin{itemize}
        \item Based on 4 past frames, predicts the next 1 frame.
        \item Videos of resolution 320x240.
        \item Baselines: CopyLastFrame, BeyondMSE and DVF\@.
    \end{itemize}
    \newpage


    \section{Folded Recurrent Neural Networks for Future Video Prediction (FRNN)}\label{sec:Folded_Recurrent_Neural_Networks_for_Future_Video_Prediction_(FRNN)}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Marc Oliu
        \item Institutions: Universitat Oberta de Catalunya, Barcelona, Spain
        \item Project website:
        \item Code: \url{https://github.com/moliusimon/frnn}
        \item Published in:
        \item Citations: 23 (As of 09-04-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Folded_Recurrent_Neural_Networks_for_Future_Video_Prediction_(FRNN):introduction}
    \begin{itemize}
        \item Motivation:
        \begin{itemize}
            \item Action and gesture recognition
            \item Task planning
            \item Weather prediction
            \item Optical flow estimation
            \item New view synthesis
        \end{itemize}
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Folded_Recurrent_Neural_Networks_for_Future_Video_Prediction_(FRNN):related-work}

    \subsection{Proposed Method}\label{subsec:Folded_Recurrent_Neural_Networks_for_Future_Video_Prediction_(FRNN):proposed-method}
    \begin{itemize}
        \item Loss Functions:
    \end{itemize}

    \subsection{Experiments}\label{subsec:Folded_Recurrent_Neural_Networks_for_Future_Video_Prediction_(FRNN):experiments}
    \begin{itemize}
        \item Datasets: Moving MNIST, UCF-101, KTH
        \item Baselines: RLadder, PredNet, Srivastava, BeyondMSE, MCnet
        \item Evaluation Metrics: MSE
    \end{itemize}
    \newpage


    \section{Probabilistic Video Generation using Holistic Attribute Control (VideoVAE)}\label{sec:Probabilistic_Video_Generation_using_Holistic_Attribute_Control_(VideoVAE)}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors:
        \item Institutions:
        Disney Research, Pittsburgh, USA;
        California Institute of Technology, Pasadena, USA;
        Simon Fraser University, Burnaby, Canada;
        The University of British Columbia, Vancouver, Canada.
        \item Project website:
        \item Code:
        \item Published in: ECCV 2018
        \item Citations: 12 (As of 26-09-2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Probabilistic_Video_Generation_using_Holistic_Attribute_Control_(VideoVAE):introduction}
    \begin{itemize}
        \item Motivation: Video Generation model are useful for building spatio-temporal priors, forecasting , and unsupervised feature learning.
        \item Contributions:
        \begin{itemize}
            \item Novel generative video model: VideoVAE
        \end{itemize}
        \item A generative video model should have the following properties:
        \begin{itemize}
            \item It should be able to model diversity of future predictions.
            \item Each future prediction, which corresponds to a sample from the generative model, should be self-consistent.
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Probabilistic_Video_Generation_using_Holistic_Attribute_Control_(VideoVAE):related-work}

    \subsection{Probabilistic Video Generation}\label{subsec:Probabilistic_Video_Generation_using_Holistic_Attribute_Control_(VideoVAE):probabilistic-video-generation}
    \begin{itemize}
        \item Loss Functions: VAE, cross-entropy loss (for holistic attribute classifiers)
        \item VAE as spatial model and LSTM as temporal model.
    \end{itemize}

    \subsection{Learning and Synthesis}\label{subsec:Probabilistic_Video_Generation_using_Holistic_Attribute_Control_(VideoVAE):learning-and-synthesis}

    \subsection{Experiments}\label{subsec:Probabilistic_Video_Generation_using_Holistic_Attribute_Control_(VideoVAE):experiments}
    \begin{itemize}
        \item Conditioned on first frame, next frames are predicted.
    \end{itemize}

    \subsubsection{Datasets}\label{subsubsec:Probabilistic_Video_Generation_using_Holistic_Attribute_Control_(VideoVAE):datasets}
    \begin{itemize}
        \item Chair CAD: Resolution 64x64.
        \item Weizmann Human Action
        \item YFCC - MIT Flickr
    \end{itemize}

    \subsubsection{Evaluation Metrics}\label{subsubsec:Probabilistic_Video_Generation_using_Holistic_Attribute_Control_(VideoVAE):evaluation-metrics}
    \begin{itemize}
        \item Inception Score: Individual Classifiers are pretrained on each of the datasets.
        \item Intra entropy and inter entropy: Individual terms in Inception Score.
    \end{itemize}

    \subsubsection{Video Synthesis}\label{subsubsec:Probabilistic_Video_Generation_using_Holistic_Attribute_Control_(VideoVAE):video-synthesis}
    \begin{itemize}
        \item Baselines: Ablation Models, Deep Rotator, VGAN, MoCoGAN\@.
    \end{itemize}
    \newpage


    \section{SDC-Net: Video prediction using spatially-displaced convolution}\label{sec:SDC_Net_Video_prediction_using_spatially_displaced_convolution}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Fitsum A Reda
        \item Institutions: NVIDIA
        \item Project website: \url{https://nv-adlr.github.io/publication/2018-SDCNet}
        \item Code:
        \item Published in: ECCV 2018
        \item Citations: 17 (As of 21-10-2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:SDC_Net_Video_prediction_using_spatially_displaced_convolution:introduction}
    \begin{itemize}
        \item Motivation: Self driving vehicles, video analysis.
        \item Contributions:
        \begin{itemize}
            \item Deep model for high-resolution frame prediction from a sequence of past frames.
            \item Spatially Displaced Convolutional (SDC) module for effective frame synthesis via transformation learning.
            \item Comparison of SDC module with kernel-based, vector-based and state-of-the-art approaches.
        \end{itemize}
        \item spatially-displaced convolutional (SDC) module is used for video frame prediction.
        A motion vector and a kernel for each pixel are learnt and a pixel is synthesized by applying the kernel at a displaced location in a source image, defined by the predicted motion vector.
    \end{itemize}

    \subsection{Method}\label{subsec:SDC_Net_Video_prediction_using_spatially_displaced_convolution:method}
    \begin{itemize}
        \item Loss Functions: $\mathcal{L}_{1} loss, $Perceptual $\mathcal{L}_1$ loss, Style loss
    \end{itemize}

    \subsection{Experiments}\label{subsec:SDC_Net_Video_prediction_using_spatially_displaced_convolution:experiments}
    \begin{itemize}
        \item Datasets: Caltech Pedestrian, YouTube-8M
        \item Baselines: Copying previous frame, Beyond MSE, PredNet, MCnet, DualGAN,
        \item Evaluation Metrics: $\mathcal{L}_1$ distance, MSE, PSNR, SSIM
    \end{itemize}
    \newpage


    \section{Stochastic Variational Video Prediction (SV2P)}\label{sec:Stochastic_Variational_Video_Prediction_(SV2P)}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Mohammad Babaeizadeh, Chelsea Finn, Sergey Levine
        \item Institutions: University of Illinois at Urbana-Champaign, University of California Berkeley, Google Brain.
        \item Project website: \url{https://sites.google.com/site/stochasticvideoprediction/}
        \item Code: \url{https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/video/sv2p.py}
        \item Published in: ICLR 2018
        \item Citations: 105 (As of 19-10-2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Stochastic_Variational_Video_Prediction_(SV2P):introduction}
    \begin{itemize}
        \item Motivation: Representations acquired by a video prediction model can be used for a variety of visual perception tasks, such as object tracking and action recognition.
        Such models will also be useful to allow an autonomous agent or robot to decide how to interact with the world to bring about a desired outcome.
        \item Contributions:
        \begin{itemize}
            \item Stochastic variational method for video prediction (SV2P)
            \item Stable training procedure for training a neural network based implementation of SV2P\@.
        \end{itemize}
        \item Modeling future distributions over images is a challenging task.
        \item Hence, it is common to make various simplifying assumptions.
        \item One particularly common assumption is that the environment is deterministic and that there is only one possible future.
        \item However real world prediction tasks are stochastic.
        \item So deterministic models predict a statistic (such as expected value) of all possible outcomes.
    \end{itemize}

    \subsection{Related Work}\label{subsec:Stochastic_Variational_Video_Prediction_(SV2P):related-work}

    \subsection{Stochastic Variational Video Prediction (SV2P)}\label{subsec:Stochastic_Variational_Video_Prediction_(SV2P):sv2p}
    \begin{itemize}
        \item Loss Functions: MLE, KL Divergence
        \item Predictions are conditioned on a set of $c$ context frames $x_0,\dots,x_{c-1}$
        \item Goal is to sample from $p(x_{c:T} | x_{0:c-1})$
        \item Prior: $z \sim p(z)$
        \item Model using the prior: \[p(x_{c:T} | x_{0:c-1}, z) = \prod_{t=c}^T p_{\theta}(x_t|x_{0:t-1}, z)\]
        \item True posterior: $p(z|x_{0:T})$ - Intractable
        \item Approximate posterior with an inference network $q_\phi(z|x_{0:T})$.
        This network outputs parameters of a conditionally gaussian distribution $\mathcal{N}(\mu_\phi(x_{0:T}), \sigma_\phi(x_{0:T}))$
        \item Inference network is trained using reparameterization trick: $z = \mu_\phi(x_{0:T}) + \sigma_\phi(x_{0:T}) \cdot \epsilon$ where $\epsilon \sim \mathcal{N}(0,I)$
        \item Optimize variational lower bound:
        \[ \mathcal{L}(x) = -\mathbb{E}_{q_\phi(z|x_{0:T})} [ \log p_\theta(x_{t:T} | x_{0:t-1}, z)] + D_{KL} (q_\phi(z|z_{0:T}) || p(z)) \]
        where prior is $p(z) = \mathcal{N}(0,I)$
        \item Approximated posterior is conditioned on all of the frames, including the future frames $x_{t:T}$.
        This is feasible during training.
        While testing, latent variables are sampled from assumed prior.
        \item Two variants:
        \begin{enumerate}
            \item Time Invariant: $z$ is sampled once for the entire video.
            \item Time Variant: $z$ is sampled once for every frame to be predicted.
            Generative model becomes
            \[ p(z_t) \prod_{t=c}^{T} p_\theta(x_t | x_{0:t-1}, z_t) \]
            and inference model becomes
            \[ q_\phi(z_t | x_{0:T})\]
        \end{enumerate}
        \item The main benefit of time-variant latent variable is better generalization beyond T, since the model does not have to encode all the events of the video in one vector z.
        \item In action-conditioned settings, we modify the generative model to be conditioned on action vector $a_t$.
        \[ p(z_t) \prod_{t=c}^{T} p_\theta(x_t | x_{0:t-1}, z_t, a_t) \]
    \end{itemize}

    \subsubsection{Model Architecture}\label{subsubsec:Stochastic_Variational_Video_Prediction_(SV2P):model-architecture}
    \begin{itemize}
        \item CNN is used for $q_\phi(z|x_{0:T})$
        \item For $p(x_t | x_{0:t-1}, z)$, CDNA architecture proposed in Unsupervised Video Prediction (Finn et al) is used.
    \end{itemize}

    \subsubsection{Training Procedure}\label{subsubsec:Stochastic_Variational_Video_Prediction_(SV2P):training-procedure}
    \begin{itemize}
        \item Training is done in 3 phases:
        \begin{enumerate}
            \item Training the generative network: Inference network is disabled.
            $z$ will be sampled from $\mathcal{N}(0,I)$
            \item Training the inference network: The inference network is trained to estimate the approximate posterior $q_\phi(z | x_{0:T})$;
            KL Loss is set to 0.
            \item Divergence reduction: KL Loss is added.
        \end{enumerate}
        \item Transition from second phase to third phase is done gradually.
    \end{itemize}

    \subsection{Stochastic Movement Dataset}\label{subsec:Stochastic_Variational_Video_Prediction_(SV2P):stochastic-movement-dataset}

    \subsection{Experiments}\label{subsec:Stochastic_Variational_Video_Prediction_(SV2P):experiments}
    \begin{itemize}
        \item Baselines: Copy previous frame, CDNA, auto-regressive stochastic model, video pixel networks (VPN)
    \end{itemize}

    \subsubsection{Datasets}\label{subsubsec:Stochastic_Variational_Video_Prediction_(SV2P):datasets}
    \begin{itemize}
        \item BAIR: 2 context frames, 10 predicted frames.
        \item Human 3.6M: Subsampled to 10fps, 10 context frames, 10 predicted frames.
        \item PUSH: 2 context frames, 10 predicted frames.
    \end{itemize}

    \subsubsection{Quantitative Comparison}\label{subsubsec:Stochastic_Variational_Video_Prediction_(SV2P):quantitative-comparison}
    \begin{itemize}
        \item PSNR, SSIM (best out of 100), Confidence of an object detector
        \item Evaluation Metrics:
    \end{itemize}

    \subsubsection{Qualitative Comparison}\label{subsubsec:Stochastic_Variational_Video_Prediction_(SV2P):qualitative-comparison}

    \subsection{Conclusion}\label{subsec:Stochastic_Variational_Video_Prediction_(SV2P):conclusion}
    \newpage


    \section{Hierarchical Long-term Video Prediction without Supervision}\label{sec:Hierarchical_Long_term_Video_Prediction_without_Supervision}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Nevan Wichers, Ruben Villegas
        \item Institutions: Google Brain, University of Michigan
        \item Project website: \url{https://sites.google.com/view/vid-pred-without-supervision/home}
        \item Code: \url{https://github.com/brain-research/long-term-video-prediction-without-supervision}
        \item Published in: ICML 2018
        \item Citations: 20 (As of 22-10-2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Hierarchical_Long_term_Video_Prediction_without_Supervision:introduction}
    \begin{itemize}
        \item Motivation: Intelligent agents
        \item Contributions:
        \begin{itemize}
            \item An unsupervised approach for discovering high-level features necessary for long-term future prediction.
            \item A joint training strategy for generating high-level features from low-level features and low-level features from high-level features simultaneously.
            \item Use of adversarial training in feature space for improved high-level feature discovery and generation.
            \item Long-term pixel-level video prediction for about 20 seconds into the future for the Human 3.6M dataset.
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Hierarchical_Long_term_Video_Prediction_without_Supervision:related-work}

    \subsection{Background}\label{subsec:Hierarchical_Long_term_Video_Prediction_without_Supervision:background}

    \subsection{Method}\label{subsec:Hierarchical_Long_term_Video_Prediction_without_Supervision:method}
    \begin{itemize}
        \item Loss Functions: $l_2$ loss, $l_2$ loss in feature space
    \end{itemize}

    \subsection{Experiments}\label{subsec:Hierarchical_Long_term_Video_Prediction_without_Supervision:experiments}
    \begin{itemize}
        \item Datasets: Human 3.6M
        \item Baselines: CDNA, SVGLP
        \item Evaluation Metrics: SSIM, 2AFC
    \end{itemize}
    \newpage


    \section{Stochastic Video Generation with a Learned Prior (SVG-LP) - ICML 2018}\label{sec:Stochastic_Video_Generation_with_a_Learned_Prior_(SVG_LP)_ICML_2018}
    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Stochastic_Video_Generation_with_a_Learned_Prior_(SVG_LP)_ICML_2018:introduction}
    \begin{itemize}
        \item \textbf{Motivation}: Learning to generate future frames of a video sequence is a challenging research problem with great relevance to reinforcement learning, planning and robotics.
        \item Uncertainty in the dynamics of the world is the main issue in video prediction.
    \end{itemize}

    \subsection{Related Work}\label{subsec:Stochastic_Video_Generation_with_a_Learned_Prior_(SVG_LP)_ICML_2018:related-work}

    \subsection{Approach}\label{subsec:Stochastic_Video_Generation_with_a_Learned_Prior_(SVG_LP)_ICML_2018:approach}
    \begin{itemize}
        \item Model has 2 components:
        \begin{itemize}
            \item Prediction model $p_\theta$: Given previous frames ($\textbf{x}_{1:t-1}$) and latent variable ($\textbf{z}_t$), predicts next frame ($\hat{\textbf{x}}_t$)
            \item Loss: MSE + $\beta$ KL[$q_\phi \Vert p$]
        \end{itemize}
    \end{itemize}
    \newpage


    \section{Video Prediction with Appearance and Motion Conditions (AMC-GAN)}\label{sec:Video_Prediction_with_Appearance_and_Motion_Conditions_(AMC_GAN)}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Yunseok Jang
        \item Institutions: University of Michigan
        \item Project website: \url{https://sites.google.com/vision.snu.ac.kr/icml2018-video-prediction}
        \item Code: \url{https://github.com/YunseokJANG/amc-gan}
        \item Published in: ICML 2018
        \item Citations: 16 (As of 09-04-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Video_Prediction_with_Appearance_and_Motion_Conditions_(AMC_GAN):introduction}
    \begin{itemize}
        \item Motivation:
        \begin{itemize}
            \item Video representation learning
        \end{itemize}
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Video_Prediction_with_Appearance_and_Motion_Conditions_(AMC_GAN):related-work}

    \subsection{Approach}\label{subsec:Video_Prediction_with_Appearance_and_Motion_Conditions_(AMC_GAN):approach}
    \begin{itemize}
        \item Loss Functions:
    \end{itemize}

    \subsection{Experiments}\label{subsec:Video_Prediction_with_Appearance_and_Motion_Conditions_(AMC_GAN):experiments}
    \begin{itemize}
        \item Datasets: MUG facial expression, NATOPS human action
        \item Baselines: CDNA, BeyondMSE, MCnet
        \item Evaluation Metrics: Motion classifier accuracy
        \item Resolution: $64 \times 64$
        \item 4 context frames, 28 predicted frames (\textcolor{blue}{to be verified}).
    \end{itemize}
    \newpage


    \section{Learning to Decompose and Disentangle Representations for Video Prediction (DDPAE)}\label{sec:Learning_to_Decompose_and_Disentangle_Representations_for_Video_Prediction_(DDPAE)}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Jun-Ting Hsieh
        \item Institutions: Stanford
        \item Project website:
        \item Code: \url{https://github.com/jthsieh/DDPAE-video-prediction}
        \item Published in: NIPS 2018
        \item Citations: 51 (As of 09-04-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Learning_to_Decompose_and_Disentangle_Representations_for_Video_Prediction_(DDPAE):introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Learning_to_Decompose_and_Disentangle_Representations_for_Video_Prediction_(DDPAE):related-work}

    \subsection{Methods}\label{subsec:Learning_to_Decompose_and_Disentangle_Representations_for_Video_Prediction_(DDPAE):methods}
    \begin{itemize}
        \item Loss Functions:
    \end{itemize}

    \subsection{Experiments}\label{subsec:Learning_to_Decompose_and_Disentangle_Representations_for_Video_Prediction_(DDPAE):experiments}
    \begin{itemize}
        \item Datasets: Moving MNIST and Bouncing Balls
        \item Baselines:
        \item Evaluation Metrics: BCE, MSE, Cosine Similarity
    \end{itemize}
    \newpage


    \section{Video Prediction via Selective Sampling (VPSS) (2018-NIPS)}\label{sec:Video_Prediction_via_Selective_Sampling_(VPSS)_(2018_NIPS)}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Jingwei Xu, Bingbing Ni, Xiaokang Yang
        \item Institutions: Shanghai Jiao Tong University
        \item Project website:
        \item Code: \url{https://github.com/xjwxjw/VPSS}.
        Tensorflow Implementation.
        \item Published in: NIPS 2018
        \item Citations: 1 (As of 05/09/2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Video_Prediction_via_Selective_Sampling_(VPSS)_(2018_NIPS):introduction}
    \begin{itemize}
        \item Motivation: Future Decision, Robot Manipulation, Autonomous driving.
        \item Using a combination of regression loss and adversarial loss results in competition between them instead of collaboration.
        \item Contributions:
        \begin{itemize}
            \item Sample multiple future frames stochastically and then choose one among them or a combination.
            This reduces blur in the predicted videos.
            \item To encourage collaboration between loss functions, design dedicated sub-networks for adversarial and regression loss respectively.
        \end{itemize}
        \item Sampling module produces multiple high quality video frame proposals.
        Trained with adversarial loss.
        \item Selection module selects high possibility candidates from proposals and combines to produce the final prediction, according to the criteria of better position matching.
        Trained with regression loss.
    \end{itemize}

    \subsection{Related Work}\label{subsec:Video_Prediction_via_Selective_Sampling_(VPSS)_(2018_NIPS):related-work}

    \subsection{Method}\label{subsec:Video_Prediction_via_Selective_Sampling_(VPSS)_(2018_NIPS):method}
    \begin{itemize}
        \item Losses used:
        \begin{itemize}
            \item Adversarial Loss
            \item Regression Loss (L2 loss)
        \end{itemize}
        \item When using a loss function as a combination of adversarial and regression loss with a balancing hyper-parameter, only one of them can be reduced i.e.\ reducing one of the loss automatically increases the other.
    \end{itemize}

    \subsubsection{Sampling Module}\label{subsubsec:Video_Prediction_via_Selective_Sampling_(VPSS)_(2018_NIPS):sampling-module}

    \subsubsection{Selection Module}\label{subsubsec:Video_Prediction_via_Selective_Sampling_(VPSS)_(2018_NIPS):selection-module}

    \subsubsection{Design Considerations and Implementation Details}\label{subsubsec:Video_Prediction_via_Selective_Sampling_(VPSS)_(2018_NIPS):design-considerations-and-implementation-details}
    \begin{itemize}
        \item Sub-networks are designed for different dedicated objectives.
        \begin{itemize}
            \item The sampler is required to produce high quality proposals without requirement of motion accuracy.
            \item Selector captures the motion information of previous inputs and select out proposals with high motion accuracy.
        \end{itemize}
        These objectives are complementary, which essentially encourages cooperation between different sub-networks.
    \end{itemize}

    \subsection{Experiments}\label{subsec:Video_Prediction_via_Selective_Sampling_(VPSS)_(2018_NIPS):experiments}

    \subsubsection{Datasets and Evaluation Setup}\label{subsubsec:Video_Prediction_via_Selective_Sampling_(VPSS)_(2018_NIPS):datasets-and-evaluation-setup}
    \begin{itemize}
        \item Datasets:
        \begin{itemize}
            \item Moving MNIST: Resolution - 64x64
            \item Robot Push: Resolution - 64x64
            \item Human 3.6M: Resolution - 64x64;
            Human subject only takes a small portion of current frame, whose motion could easily be ignored only with the regression loss.
        \end{itemize}
        \item Baselines and other models:
        \begin{itemize}
            \item Dynamic Filter Networks (DFN)
            \item CDNA
            \item DrNet
            \item MCNet
            \item Stochastic Variational Video Prediction (SV2P)
            \item Stochastic Video Generation with Learned Prior (SVG-LP)
        \end{itemize}
    \end{itemize}

    \subsubsection{Quantitative Evaluation}\label{subsubsec:Video_Prediction_via_Selective_Sampling_(VPSS)_(2018_NIPS):quantitative-evaluation}
    \begin{itemize}
        \item PSNR
        \item SSIM
        \item Inception Score
    \end{itemize}

    \subsubsection{Qualitative Evaluation}\label{subsubsec:Video_Prediction_via_Selective_Sampling_(VPSS)_(2018_NIPS):qualitative-evaluation}
    \begin{itemize}
        \item 1270 prediction results of each dataset.
        \item 40 subjects.
        \item 3 aspects:
        \begin{enumerate}
            \item Regarding the real video samples as baseline, which one is more realistic (Not based on previous inputs)?
            \item Considering image quality and previous inputs, which one is more similar to the Ground Truth?
            \item Considering motion accuracy and previous inputs, which one is more similar to the Ground Truth?
        \end{enumerate}
    \end{itemize}

    \subsubsection{Discussion}\label{subsubsec:Video_Prediction_via_Selective_Sampling_(VPSS)_(2018_NIPS):discussion}

    \subsection{Conclusion}\label{subsec:Video_Prediction_via_Selective_Sampling_(VPSS)_(2018_NIPS):conclusion}
    \newpage


    \section{FutureGAN: Anticipating the Future Frames of Video Sequences using Spatio-Temporal 3d Convolutions in Progressively Growing GANs}\label{sec:FutureGAN_Anticipating_the_Future_Frames_of_Video_Sequences_using_Spatio_Temporal_3d_Convolutions_in_Progressively_Growing_GANs}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Sandra Aigner, Marco Korner
        \item Institutions: Technical University of Munich
        \item Project website:
        \item Code: \url{https://github.com/TUM-LMF/FutureGAN}
        \item Published in: 2018
        \item Citations: 5 (As of 22-10-2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:FutureGAN_Anticipating_the_Future_Frames_of_Video_Sequences_using_Spatio_Temporal_3d_Convolutions_in_Progressively_Growing_GANs:introduction}
    \begin{itemize}
        \item Motivation: Robotics, autonomous driving, learning video representations.
        \item Contributions:
        \begin{itemize}
            \item GAN based model for Video Prediction, which predicts multiple future frames at once.
        \end{itemize}
        \item Uses Progressively Growing GANs (PG-GANs)
    \end{itemize}

    \subsection{Related Work}\label{subsec:FutureGAN_Anticipating_the_Future_Frames_of_Video_Sequences_using_Spatio_Temporal_3d_Convolutions_in_Progressively_Growing_GANs:related-work}

    \subsection{FutureGAN Model}\label{subsec:FutureGAN_Anticipating_the_Future_Frames_of_Video_Sequences_using_Spatio_Temporal_3d_Convolutions_in_Progressively_Growing_GANs:futuregan-model}
    \begin{itemize}
        \item Loss Functions: WGAN Loss, gradient-penalty, epsilon-penalty
    \end{itemize}

    \subsection{Experiments}\label{subsec:FutureGAN_Anticipating_the_Future_Frames_of_Video_Sequences_using_Spatio_Temporal_3d_Convolutions_in_Progressively_Growing_GANs:experiments}
    \begin{itemize}
        \item Datasets: Moving MNIST, KTH, Cityscapes
        \item Baselines: Copying previous frame, FRNN, MCnet
        \item Evaluation Metrics: MSE, PSNR, SSIM
        \item On KTH dataset, prediction is done upto 120 steps.
    \end{itemize}
    \newpage


    \section{Reduced-Gate Convolutional LSTM Using Predictive Coding for Spatiotemporal Prediction}\label{sec:Reduced_Gate_Convolutional_LSTM_Using_Predictive_Coding_for_Spatiotemporal_Prediction}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Nelly Elsayed
        \item Institutions:
        \item Project website:
        \item Code: \url{https://github.com/NellyElsayed/rgcLSTM}
        \item Published in: arXiv 2018, ICLR 2019 Reject
        \item Citations: 3 (As of 21-10-2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Reduced_Gate_Convolutional_LSTM_Using_Predictive_Coding_for_Spatiotemporal_Prediction:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Reduced_Gate_Convolutional_LSTM_Using_Predictive_Coding_for_Spatiotemporal_Prediction:related-work}

    \subsection{Method}\label{subsec:Reduced_Gate_Convolutional_LSTM_Using_Predictive_Coding_for_Spatiotemporal_Prediction:method}
    \begin{itemize}
        \item Loss Functions:
    \end{itemize}

    \subsection{Experiments}\label{subsec:Reduced_Gate_Convolutional_LSTM_Using_Predictive_Coding_for_Spatiotemporal_Prediction:experiments}
    \begin{itemize}
        \item Datasets: Moving MNIST, KITTI
        \item Baselines: PredNet, FC-LSTM, CDNA, DFN, VPN, ConvLSTM, ConvGRU, TrajGRU, PredRNN
        \item Evaluation Metrics: MSE, MAE, SSIM
    \end{itemize}
    \newpage


    \section{Stochastic Adversarial Video Prediction (SAVP)}\label{sec:Stochastic_Adversarial_Video_Prediction_(SAVP)}
    \subsection*{Abstract}
    \begin{itemize}
        \item Learning to predict raw future observations, such as frames in a video, is exceedingly challenging|the ambiguous nature of the problem can cause a naively designed model to average together possible futures into a single, blurry prediction.
        \item Two recent approaches:
        \begin{itemize}
            \item Latent variational variable models that explicitly model underlying stochasticity (VAEs).
            Doesn't produce realistic results.
            \item Adversarially-trained models that aim to produce naturalistic images (GANs).
            Doesn't produce diverse predictions.
        \end{itemize}
        \item Combine these 2 complementary approaches.
        \item Code:
        \begin{itemize}
            \item \url{https://alexlee-gk.github.io/video_prediction}
            \item \url{https://github.com/alexlee-gk/video_prediction}
        \end{itemize}
    \end{itemize}

    \subsection{Introduction}\label{subsec:Stochastic_Adversarial_Video_Prediction_(SAVP):introduction}
    \begin{itemize}
        \item \textbf{Motivation}: The ability to imagine future outcomes provides an appealing avenue for learning about the world.
        Unlabeled video sequences can be gathered autonomously with minimal human intervention, and a machine that learns to accurately predict future events will have gained an in-depth and functional understanding of its physical environment.
        \item Once trained, such a video prediction model could be used to determine which actions can bring about desired outcomes.
        \item Ambiguity in future makes the problem multimodel.
        \item Using MSE as loss function and a deterministic model leads to averaging of possible futures and hence produce blurry predictions.
        \item VAEs optimize a variational lower bound on the likelihood of the data in a latent variable model.
        However, the posterior is still a pixel-wise MSE loss, corresponding to the log-likelihood under a fully factorized Gaussian distribution.
        This makes training tractable, but causes them to still make blurry and unrealistic predictions when the latent variables alone do not adequately capture the uncertainty.
        \item GANs are notoriously susceptible to mode collapse, where latent random variables are often ignored by the model, especially in the conditional setting.
        Hence, diversity is lost.
        \item Our model consists of a video prediction network that can sample multiple plausible futures by sampling time-varying stochastic latent variables and decoding them into multiple frames.
        \item At training time, an inference network estimates the distribution of these latent variables, and video discriminator networks classify generated videos from real.
        \item The full training objective is the variational lower bound used in VAEs combined with the adversarial loss used in GANs. This enables to capture stochastic posterior distributions of videos while also modeling the spatiotemporal joint distribution of pixels.
        \item SSIM was not designed to handle spatial ambiguities.
    \end{itemize}

    \subsection{Related Work}\label{subsec:Stochastic_Adversarial_Video_Prediction_(SAVP):related-work}
    \begin{itemize}
        \item 3 types of video prediction: Deterministic, low stochastic (stochasticity introduced only by noise), fully stochastic (latent vector)
    \end{itemize}

    \subsection{Video Prediction with Stochastic Adversarial Models}\label{subsec:Stochastic_Adversarial_Video_Prediction_(SAVP):video-prediction-with-stochastic-adversarial-models}
    \begin{itemize}
        \item Our model consists of a recurrent generator network G, which is a deterministic video prediction model that maps an initial image $x_0$ and a sequence of latent random codes $z_{0:T-1}$ to the predicted sequence of future images $\hat{x}_{1:T}$
        \item Intuitively, the latent codes encapsulate any ambiguous or stochastic events that might affect the future.
        \item At test time, we sample videos by first sampling the latent codes from a prior distribution $p(z_t)$, and then passing them to the generator.
        We use a fixed unit Gaussian prior, $\mathcal{N}(0; 1)$.
        \item 64x64 frames.
        About 28--30 frames are predicted.
        \item Loss equations:
        \begin{align*}
            \mathcal{L}_1(G,E) &= \mathbb{E}_{\textbf{x}_{0:T}, \textbf{z}_t \sim \textit{E}(\textbf{x}_{t:t+1}) \vert_{t=0}^{T-1}} \left[ \sum_{t=1}^{T} \Vert \textbf{x}_t - G(\textbf{x}_0, \textbf{z}_{0:t-1}) \Vert_1 \right] \\
            \mathcal{L}_{\textrm{KL}}(E) &= \mathbb{E}_{\textbf{x}_{0:T}} \left[ \sum_{t=1}^{T} \mathcal{D}_{\textrm{KL}}(\textit{E}(\textbf{x}_{t-1:t}) \Vert p(\textbf{z}_{t-1})) \right] \\
            G^*, E^* &= \argmin_{G,E} \lambda_1 \mathcal{L}_1(G,E) + \lambda_{\textrm{KL}} \mathcal{L}_{\textrm{KL}}(\textit{E}) \\
            \mathcal{L}_{\textrm{GAN}}(G,D) &= \mathbb{E}_{x_{1:T}}\left[ \textrm{log} D(x_{0:T-1}) \right] + \mathbb{E}_{x_{1:T}, z_t \sim p(z_t) \vert_{t=0}^{T-1}} \left[ \textrm{log} (1-D(G(x_0, z_{0:T-1})))\right] \\
            G^* &= \argmin_G \max_D \mathcal{L}_{\textrm{GAN}}(G,D) \\
            G^*, E^* &= \argmin_{G,E} \max_{D, D^{\textrm{VAE}}} \lambda_1 \mathcal{L}_1(G,E) + \lambda_{\textrm{KL}} \mathcal{L}_{\textrm{KL}}(E) + \mathcal{L}_{\textrm{GAN}}(G,D) + \mathcal{L}_{\textrm{GAN}}^{\textrm{VAE}}(G,E,D^{\textrm{VAE}}) \\
        \end{align*}
    \end{itemize}

    \subsection{Experiments}\label{subsec:Stochastic_Adversarial_Video_Prediction_(SAVP):experiments}

    \subsubsection{Evaluation Metrics}\label{subsubsec:Stochastic_Adversarial_Video_Prediction_(SAVP):evaluation-metrics}
    Study of realism, diversity, and accuracy of the generated videos.
    \begin{itemize}
        \item Realism: 2AFC (Two Alternative Forced Choice) test: Humans used as discriminators.
        Given a real and fake video, humans have to identify the real video.
        \item Diversity: Average distance between randomly sampled video predictions.
        Distance is measured in VGG space, (pretrained VGG model for ImageNet classification) averaged across five layers. \textbf{Todo: Check which layers from the code}
        \item Accuracy: The model is sampled a finite number of times (100 samples).
        The similarity between the best sample and ground truth is evaluated.
        Along with PSNR and SSIM, cosine similarity in pretrained VGG feature space is also used.
        \begin{itemize}
            \item Why this metric?: One short-coming of diversity is that, even if predictions are diverse, we may not cover the entire feasible output space.
            Since we don't know the true distribution, we have no way of verifying if the predictions have convered the entire feasible output space.
            But since we have a reference video, we can check if one of the predictions is the reference video or not.
            So, we sample a finite video predictions and compare the closest resembling one with the reference video.
            If none of the predictions are close to the reference video, then this implies that we have not covered the entire feasible output space.
            \item Cosine Similarity: Angle between two vectors: $\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{\vert|\vec{a}\vert|\ \vert|\vec{b}\vert|}$. \\
            \textbf{Todo: Check how it is done in code}
            \item Generalization ability is tested by running the model for more time steps than it was trained for.
        \end{itemize}
    \end{itemize}

    \subsubsection{Datasets}\label{subsubsec:Stochastic_Adversarial_Video_Prediction_(SAVP):datasets}
    \begin{itemize}
        \item BAIR action-free robot pushing dataset and KTH human actions dataset are used.
        \item BAIR:
        \begin{itemize}
            \item Randomly moving robotic arm.
            \item Resolution: 64x64
            \item Conditioned on first 2 frames, next frames are predicted.
            \item 10 frames are predicted for 2AFC test.
            \item 28 frames are predicted for other experiments.
        \end{itemize}
        \item KTH:
        \begin{itemize}
            \item Frame resolution: 120x160.
            Frame rate: 25fps.
            \item Preprocessing: Center-crop each frame to a 120x120 square and then resize to a spatial resolution of 64x64.
            \item Conditioned on first 10 frames, next frames are predicted.
            \item 10 frames are predicted for 2AFC test.
            \item 30 frames are predicted for other experiments.
            \item 6 activities: walking, jogging, running, boxing, hand-waving, hand-clapping
            \item Stochasiticity is obtained only when all initial frames are empty.
            Stochasticity lies in what time human enters the frame.
        \end{itemize}
    \end{itemize}

    \subsubsection{Methods: Ablations and Comparisons}\label{subsubsec:Stochastic_Adversarial_Video_Prediction_(SAVP):ablations}
    \begin{itemize}
        \item Ablations:
        \begin{itemize}
            \item GAN-only
            \item VAE-only
            \item deterministic
        \end{itemize}
        \item Comparisons:
        \begin{itemize}
            \item Stochastic Variational Video Prediction (SV2P)
            \item Stochastic Video Generation with a Learned Prior (SVG-LP)
            \item MoCoGAN
        \end{itemize}
    \end{itemize}

    \subsubsection{Experimental Results}\label{subsubsec:Stochastic_Adversarial_Video_Prediction_(SAVP):experimental-results}
    \begin{itemize}
        \item GAN based variants get high realism score but diversity is low.
        VAE based variants get high diversity but realism score is low.
        \item SV2P was not evaluated on KTH dataset.
        So, hyperparameters are not optimal.
        \item VGG similarity has been shown to better match human perceptual judgments.
        \item The general trend is that models trained with $\mathcal{L}_2$, which favors blurry predictions, are better on PSNR and SSIM, but models trained with $\mathcal{L}_1$ are better on VGG cosine similarity.
        \item We expect for our GAN-based variants to underperform on PSNR and SSIM since GANs prioritize matching joint distributions of pixels over per-pixel reconstruction accuracy.
        \item VGG similarity is a held-out metric, meaning that it was not used as a loss function during training.
        \item On stochastic environments (BAIR action free), there is some correlation between diversity and accuracy - a model with diverse prediction is more likely to sample a video that is close to ground truth.
        \item On less stochastic environments (KTH conditioned on 10 frames), the above correlation doesn't hold.
        \item KTH dataset conditioned on 10 frames is less stochastic because there is not much difference between deterministic and savp models.
        \item On stochastic datasets, adding GAN to VAE model doesn't reduce diversity, but improves realism.
    \end{itemize}
    \newpage


    \section{Stochastic Dynamics for Video Infilling (SDVI)}\label{sec:Stochastic_Dynamics_for_Video_Infilling_(SDVI)}
    \subsection*{Abstract}
    \begin{itemize}
        \item Stochastic generation framework to infill long intervals of video sequences.
        \item Video interpolation aims to produce transitional frames for a short interval between every two frames.
        Video Infilling, however, aims to complete long intervals in a video sequence.
    \end{itemize}

    \subsection{Introduction}\label{subsec:Stochastic_Dynamics_for_Video_Infilling_(SDVI):introduction}
    \begin{itemize}
        \item Video interpolation is used for videos with frame rate above 20fps.
        This paper focuses on long-term interval filling for videos with frame rate less than 3fps.
        \item Application: Low frame rate camera with limited memory.
        \item Uncertainties and randomness in long-term interval is greater compared to short-term intervals.
        \item Utilizing long-term information can benefit the dynamics inference by eliminating uncertainties.
        \item In this paper, motion dynamics is modelled same as video prediction.
        It has 2 major challenges:
        \begin{itemize}
            \item Unlike the video prediction, the temporal layout of our input is bi-directional with temporal gaps.
            \item Interpolation requires coherency between the generated sequence and reference frames from both directions.
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Stochastic_Dynamics_for_Video_Infilling_(SDVI):related-work}

    \subsection{Methods}\label{subsec:Stochastic_Dynamics_for_Video_Infilling_(SDVI):methods}

    \subsection{Experiments}\label{subsec:Stochastic_Dynamics_for_Video_Infilling_(SDVI):experiments}
    \begin{itemize}
        \item 4 datasets: Stochastic Moving MNIST (SM-MNIST), KTH action database, BAIR robot pushing dataset and UCF101.
        \item \textcolor{blue}{LMS: Last Momentum Similarity: Mean Squared distance between optical flow from $X_{T-1}$ to $X_T$ and the optical flow from $\tilde{X}_{T-1}^{infr}$ to $X_t$}
        \item Ablation studies are conducted by removing the spatial sampling or the extended reference frames in $\mathbf{X_{WR}}$.
        \item \textcolor{red}{\textbf{Code not yet released.
        Code will be released after the paper review process.} }
    \end{itemize}
    \newpage


    \section{TGANv2: Efficient Training of Large Models for Video Generation with Multiple Subsampling Layers}\label{sec:TGANv2_Efficient_Training_of_Large_Models_for_Video_Generation_with_Multiple_Subsampling_Layers}
    \subsection*{Abstract}
    \begin{itemize}
        \item Datasets: UCF101, FaceForensics
        \item Resolution: 192x192 and 256x256.
        \item 16 frames generated.
    \end{itemize}
    \newpage


    \section{Quality Assessment of In-the-Wild Videos}\label{sec:Quality_Assessment_of_In_the_Wild_Videos}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Dingquan Li, Tingting Jiang, Ming Jiang
        \item Institutions: Peking University
        \item Project website:
        \item Code: \url{https://github.com/lidq92/VSFA}
        \item Published in: ACM Conference on Multi-Media 2019
        \item Citations: 0 (As of 24/12/2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Quality_Assessment_of_In_the_Wild_Videos:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Quality_Assessment_of_In_the_Wild_Videos:related-work}

    \subsection{The Proposed Method}\label{subsec:Quality_Assessment_of_In_the_Wild_Videos:the-proposed-method}
    \begin{itemize}
        \item Main components
        \begin{itemize}
            \item Content Aware feature extraction using ResNet
            \item Mean and Standard Deviation pooling
            \item Dimensionality Reduction using Fully Connected Layers
            \item GRU for long-term dependencies
            \item Subjectively inspired temporal pooling
        \end{itemize}
    \end{itemize}

    \subsection{Experiments}\label{subsec:Quality_Assessment_of_In_the_Wild_Videos:experiments}
    \newpage


    \section{Order Matters: Shuffling Sequence Generation for Video Prediction (SEE Net)}\label{sec:Order_Matters_Shuffling_Sequence_Generation_for_Video_Prediction_(SEE_Net)}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Junyan Wang
        \item Institutions: Newcastle University, UK\@.
        \item Project website:
        \item Code: \url{https://github.com/andrewjywang/SEENet}
        \item Published in: BMVC 2019
        \item Citations: 0 (As of 20-10-2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Order_Matters_Shuffling_Sequence_Generation_for_Video_Prediction_(SEE_Net):introduction}
    \begin{itemize}
        \item Motivation: Robotics and healthcare
        \item Contributions:
        \begin{itemize}
            \item SEE Net for long-term future frame prediction.
            \item Shuffle discriminator to explicitly control the extraction of sequential information.
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Order_Matters_Shuffling_Sequence_Generation_for_Video_Prediction_(SEE_Net):related-work}

    \subsection{Problem Statement}\label{subsec:Order_Matters_Shuffling_Sequence_Generation_for_Video_Prediction_(SEE_Net):problem-statement}

    \subsection{SEE-Net}\label{subsec:Order_Matters_Shuffling_Sequence_Generation_for_Video_Prediction_(SEE_Net):see-net}
    \begin{itemize}
        \item Loss Functions: Adversarial Loss, Shuffle Loss, Consistency Loss.
        \item Optical Flow between frames is fed to the network.
        PWCNet is used to obtain Optical Flow.
    \end{itemize}

    \subsection{Experiments}\label{subsec:Order_Matters_Shuffling_Sequence_Generation_for_Video_Prediction_(SEE_Net):experiments}
    \begin{itemize}
        \item Datasets: Moving MNIST, KTH, MSR
        \item Baselines: MCnet, DrNet
        \item Evaluation Metrics: PSNR, SSIM
    \end{itemize}
    \newpage


    \section{Predicting Future Frames using Retrospective Cycle GAN}\label{sec:Predicting_Future_Frames_using_Retrospective_Cycle_GAN}
    \subsection*{Paper details}
    \begin{itemize}
        \item Authors: Yong-Hoon Kwon, Min-Gyu Park
        \item Institutions: LG Electronics
        \item Project website:
        \item Code:
        \item Published in: CVPR 2019
        \item Citations: 1 (As of 20-10-2019)
    \end{itemize}
    \subsection*{Abstract}
    \begin{itemize}
        \item The key idea is to train a single generator that can predict both future and past frames while enforcing the consistency of bi-directional prediction using the retrospective cycle constraints.
    \end{itemize}

    \subsection{Introduction}\label{subsec:Predicting_Future_Frames_using_Retrospective_Cycle_GAN:introduction}
    \begin{itemize}
        \item Motivation: Abnormal event detection, video coding, video completion, robotics, and autonomous driving.
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Predicting_Future_Frames_using_Retrospective_Cycle_GAN:related-work}

    \subsection{Proposed Method}\label{subsec:Predicting_Future_Frames_using_Retrospective_Cycle_GAN:proposed-method}
    \begin{itemize}
        \item Loss Functions: L1 loss between frames, L1 loss between LoG of frames, frame adversarial loss, sequence adversarial loss.
    \end{itemize}

    \subsection{Experimental Results}\label{subsec:Predicting_Future_Frames_using_Retrospective_Cycle_GAN:experimental-results}
    \begin{itemize}
        \item Datasets: KITTI, Caltech Pedestrian, UCF101, Surveillance Videos
        \item Baselines: Copying previous frame, PredNet, DM-GAN, BeyondMSE, ContextVP, MCnet+RES, EpicFlow, DVF
        \item Evaluation Metrics: MSE, PSNR, SSIM
    \end{itemize}

    \subsubsection{Datasets}\label{subsubsec:Predicting_Future_Frames_using_Retrospective_Cycle_GAN:datasets}

    \subsubsection{Training Details}\label{subsubsec:Predicting_Future_Frames_using_Retrospective_Cycle_GAN:training-details}

    \subsubsection{Quantitative and qualitative evaluation}\label{subsubsec:Predicting_Future_Frames_using_Retrospective_Cycle_GAN:quantitative-and-qualitative-evaluation}

    \subsubsection{Multi-step prediction evaluation}\label{subsubsec:Predicting_Future_Frames_using_Retrospective_Cycle_GAN:multi-step-prediction-evaluation}
    \begin{itemize}
        \item Trained to predict 1 frame.
        Can predict upto 15 frames.
    \end{itemize}

    \subsection{Conclusion}\label{subsec:Predicting_Future_Frames_using_Retrospective_Cycle_GAN:conclusion}
    \newpage


    \section{Disentangling Propagation and Generation for Video Prediction}\label{sec:Disentangling_Propagation_and_Generation_for_Video_Prediction}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Hang Gao, Fisher Yu, Trevor Darrell
        \item Institutions: UC Berkeley
        \item Project website:
        \item Code: \url{https://github.com/Fangyh09/Disentangling-Propagation-and-Generation-for-Video-Prediction}
        \item Published in: ICCV 2019
        \item Citations: 10 (As of 11-05-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Disentangling_Propagation_and_Generation_for_Video_Prediction:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Disentangling_Propagation_and_Generation_for_Video_Prediction:related-work}

    \subsection{Method}\label{subsec:Disentangling_Propagation_and_Generation_for_Video_Prediction:method}
    \begin{itemize}
        \item Loss Functions:
    \end{itemize}

    \subsection{Experiments}\label{subsec:Disentangling_Propagation_and_Generation_for_Video_Prediction:experiments}
    \begin{itemize}
        \item Datasets:
        \item Baselines:
        \item Evaluation Metrics:
    \end{itemize}

    \subsection{My Summary}\label{subsec:Disentangling_Propagation_and_Generation_for_Video_Prediction:my-summary}
    Predict optical flow and warp the last frame to get a coarse next frame.
    From this, generate a confidence map, which gives low confidence in disoccluded areas.
    Use an Generator (inpainting module) to fill in disoccluded areas.
    \newpage


    \section{Spatio-Temporal Measures Of Naturalness}\label{sec:Spatio_Temporal_Measures_Of_Naturalness}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Zeina Sinno, Alan Bovik
        \item Institutions: UT Austin
        \item Project website:
        \item Code:
        \item Published in: ICIP 2019
        \item Citations: 0 (As of 10-02-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Spatio_Temporal_Measures_Of_Naturalness:introduction}
    \begin{itemize}
        \item Early visual system is shaped to properties of the natural environment, including important statistical regularities.
        \item Many VQA models leverage the fact that undistorted videos present statistical regularities that are systematically and predictably degraded by distortions.
        \item Microsaccades are not studied yet.
        \item Bandpass processed natural images have highly regular first and second-order statistics, which are perturbed by distortions.
        \item This paper modeled statistics of frame differences of natural videos that are oriented in (spatially displaced) space-time.
        \item This paper devised space-time directional natural video statistics (NVS)
    \end{itemize}

    \subsubsection{Space-Time Directional Models}\label{subsubsec:Spatio_Temporal_Measures_Of_Naturalness:space-time-directional-models}
    \begin{itemize}
        \item Four directional temporal differences:
        \begin{align}
            D_H(i,j)_t &= I_t(i,j) - I_{t+1}(i, j-1) \\
            D_V(i,j)_t &= I_t(i,j) - I_{t+1}(i-1, j) \\
            D_{D_1}(i,j)_t &= I_t(i,j) - I_{t+1}(i-1, j-1) \\
            D_{D_2}(i,j)_t &= I_t(i,j) - I_{t+1}(i-1, j+1)
        \end{align}
        \item MSCN coefficients are computed with a 7 x 7 window, and Gaussian weights (set to $3 \sigma$)
        \item $96 \times 96$ patches are used
        \item The empirical distribution (histogram) of the directional MSCN coefficients from each patch are fit to a zero-mean GGD by Method of Moments (MoM).
        This is done for each of the space-time directions.
        \item For a given video, 4 directional NVS models are obtained.
        Each vector NVS model is of size $p \times 2$, where $p$ is the total number of patches in the video.
        \item Each model is obtained on the Luminance frame of the video.
        \item This is done on a collection of high quality videos, selected by a small Subjective Study.
        \item Since most of the videos are of resolution $1920 \times 1080$ and consumer devices can produce wide range of video resolutions, randomly selected videos were downscaled to one of $1280 \times 720$, $960 \times 640$ and $640 \times 360$.
        \item There are about 50 videos for each resolution.
        \item Any video longer than 10 seconds is trimmed to 10s.
        \item For higher resolution videos ($1920 \times 1080$ and $1280 \times 720$), only 5\% of the patches are selected based on sharpness.
        For lower resolution videos ($960 \times 640$ and $640 \times 360$), 25\% of the patches are selected.
        \item The best fitting parameters ($\alpha$,$\beta$) were collected and horizontally aggregated over space and time across all videos for each of the four orientations, yielding 4 pristine models $P_H$, $P_V$ , $P_{D_1}$ and $P_{D_2}$.
        \item Mahalanobis distance is computed between the each directional pristine model and the corresponding directional input video models.
        Higher the distance, lower the quality.
    \end{itemize}

    \subsection{Is Directional Naturalness a Good Predictive Measure of Quality?}\label{subsec:Spatio_Temporal_Measures_Of_Naturalness:is-directional-naturalness-a-good-predictive-measure-of-quality?}
    \begin{itemize}
        \item PLCC, SROCC and RMSE are computed between negative values of the distances and MOS\@.
        For PLCC and RMSE, non-linear mapping is applied.
        \item A scatter-plot of MOS v/s these distances is also plotted.
        \item Results:
        \begin{itemize}
            \item PLCC $\approx 0.6$
            \item SROCC $\approx 0.6$
            \item RMSE $\approx 13.58$
        \end{itemize}
    \end{itemize}
    \newpage


    \section{Bounce and Learn - Modeling Scene Dynamics with Real-World Bounces (ICLR 2019, CMU)}\label{sec:Bounce_and_Learn_Modeling_Scene_Dynamics_with_Real_World_Bounces_(ICLR_2019,_CMU)}
    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Bounce_and_Learn_Modeling_Scene_Dynamics_with_Real_World_Bounces_(ICLR_2019,_CMU):introduction}
    \begin{itemize}
        \item \textbf{Motivation - Robotics and Augmented Reality}: The ability for a system to make such predictions will allow applications
        in augmented reality and robotics, such as compositing a dynamic virtual object into a video or
        allowing an agent to react to real-world bounces in everyday environments.
        \item From videos, physical properties of the surfaces are approximated.
        This is then used as supervision to learn an appearance-based estimator.
        \item \textbf{Goal}:
        \begin{itemize}
            \item Predict post-bounce trajectories
            \item Estimate surface-varying coefficients of restitution (COR) and effective collision normals.
        \end{itemize}
        \item To model collision events, often rigid-body physics are used.
        But during collision real-world objects deform and hence violate rigid-body assumptions.
        \item \textbf{Contributions}:
        \begin{itemize}
            \item Predicting post bounce trajectories
            \item Inferring physical properties (COR and collision normal) from single still image
            \item Bounce Dataset: large-scale dataset of real-world bounces in a variety of everyday scenes
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Bounce_and_Learn_Modeling_Scene_Dynamics_with_Real_World_Bounces_(ICLR_2019,_CMU):related-work}

    \subsection{Bounce and Learn Model and Bounce Dataset}\label{subsec:Bounce_and_Learn_Modeling_Scene_Dynamics_with_Real_World_Bounces_(ICLR_2019,_CMU):dataset}

    \subsubsection{Physics Inference Module (PIM)}\label{subsubsec:Bounce_and_Learn_Modeling_Scene_Dynamics_with_Real_World_Bounces_(ICLR_2019,_CMU):physics-inference-modulepim}
    \begin{itemize}
        \item Physics Inference Module (PIM) is pretrained on simulation data
        \item Input: 10 frames;
        Output: 10 frames
        \item Encode past 10 frames into a vector.
        Using physical properties, predict the output vector.
        Decode the output vector into frames.
        \item Training Loss for PIM: Distance between encodings of predicted and actual trajectory (after collision).
        Note: There are other terms as well.
    \end{itemize}

    \subsubsection{Visual Inference Module (VIM)}\label{subsubsec:Bounce_and_Learn_Modeling_Scene_Dynamics_with_Real_World_Bounces_(ICLR_2019,_CMU):vim}
    \begin{itemize}
        \item Visual Inference Module (VIM) takes input as as the image of a scene and outputs the physical parameters for each location in the scene.
        \item VIM is trained end-to-end along with PIM\@.
        \item Online Learning: Estimates of physical parameters in a scene are updated online upon observing bounces.
        This is useful in robotics where an agent can interact with environment to infer the physical properties.
    \end{itemize}

    \subsubsection{Bounce Dataset}\label{subsubsec:Bounce_and_Learn_Modeling_Scene_Dynamics_with_Real_World_Bounces_(ICLR_2019,_CMU):bounce-dataset}
    \begin{itemize}
        \item Real-world Bounce Dataset:
        \begin{itemize}
            \item 5172 stereo videos
            \item On average, each video contains 172 frames with the ball.
            \item Each sample in the dataset consists of the RGB frames, depth maps, point clouds for each frame, and estimated surface normal maps.
        \end{itemize}
        \item Simulation Data:
        \begin{itemize}
            \item Simulate a set of sphere-to-plane collisions with the PyBullet Physics Engine
        \end{itemize}
    \end{itemize}

    \subsection{Evaluation}\label{subsec:Bounce_and_Learn_Modeling_Scene_Dynamics_with_Real_World_Bounces_(ICLR_2019,_CMU):evaluation}

    \subsubsection{Visual Forward Prediction}\label{subsubsec:Bounce_and_Learn_Modeling_Scene_Dynamics_with_Real_World_Bounces_(ICLR_2019,_CMU):visual-forward-prediction}
    \begin{itemize}
        \item Split of dataset: No common scenes across the splits.
        \begin{itemize}
            \item Training: 4503
            \item Validation: 196
            \item Test: 473
        \end{itemize}
        \item Mix of 3:1 synthetic-to-real data in the mini-batches.
        \item Quantitative Evaluation: $\mathcal{L}_2$ distance between world coordinates of ball center of the predicted and ground truth at time step 0.1 seconds post-bounce.
        \item Baselines:
        \begin{enumerate}
            \item Replace PointNet encoder with `center-encoding' model
            \item Parabola encoding: Fit a parabola to ball centre positions in a Least-Squares sense.
            Parameters of this parabola are input to a neural net.
        \end{enumerate}
    \end{itemize}
    \newpage


    \section{FVD - A new Metric for Video Generation}\label{sec:FVD_A_new_Metric_for_Video_Generation}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Thomas Unterthiner, \ldots
        \item Institutions: Google Brain
        \item Project website:
        \item Code:
        \item Published in: ICLR 2019 (Workshop paper)
        \item Citations: 0 (As of 13-02-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:FVD_A_new_Metric_for_Video_Generation:introduction}
    \begin{itemize}
        \item Generative models of video will enable many applications, including missing-frame prediction, improved instance segmentation, or complex (relational) reasoning tasks by conducting inference.
        \item FVD builds on principles of FID\@.
    \end{itemize}

    \subsection{Frechet Video Distance}\label{subsec:FVD_A_new_Metric_for_Video_Generation:frechet-video-distance}
    \begin{itemize}
        \item Code: \url{https://git.io/fpuEH}
        \item It is difficult to solve Frechet Distance for general distributions, but for multivariate Gaussians, it has a closed for solution.
        \begin{align*}
            d(P_R, P_G) = \vert \mu_R - \mu_G \vert ^2 + \textrm{Tr} \left( \Sigma_R + \Sigma_G - 2(\Sigma_R \Sigma_G)^\frac{1}{2} \right)
        \end{align*}
        \item The multivariate Gaussian is a reasonable approximation in a suitable feature space.
        \item Inflated 3D ConvNet (I3D) is used to extract features.
        Frechet distance on these features gives FVD\@.
        \item I3D network generalizes Inception architecture to sequential data and is trained to perform action recognition on Kinetics dataset.
        \item Kernel Video Distance (KVD): Uses Maximum Mean Discrepancy (instead of Frechet distance).
        Polynomial kernel is used: $\kappa(a,b) := \left(a^T b+1\right)^3$
    \end{itemize}

    \subsection{Experiments}\label{subsec:FVD_A_new_Metric_for_Video_Generation:experiments}

    \subsubsection{Noise Study}\label{subsubsec:FVD_A_new_Metric_for_Video_Generation:noise-study}
    \begin{itemize}
        \item Static and temporal noise are added at 6 different levels.
        \item Videos from BAIR, Kinetics-400 and HMDB51
    \end{itemize}

    \subsubsection{Human Evaluation}\label{subsubsec:FVD_A_new_Metric_for_Video_Generation:human-evaluation}
    \begin{itemize}
        \item Where other metrics like PSNR and SSIM cannot distinguish between good models, FVD can reliably distinguish.
        \item No other metric can reliably distinguish between good models that are equal in terms of FVD\@.
        \item No other metric can improve upon the ranking induced by FVD\@.
    \end{itemize}

    \subsection{Conclusion}\label{subsec:FVD_A_new_Metric_for_Video_Generation:conclusion}

    \subsection{Appendix}\label{subsec:FVD_A_new_Metric_for_Video_Generation:appendix}

    \subsubsection{Appendix A: Noise Study}\label{subsubsec:FVD_A_new_Metric_for_Video_Generation:appendix-a:-noise-study}
    \begin{itemize}
        \item Static noise:
        \begin{itemize}
            \item Black rectangle
            \item Gaussian blur
            \item Gaussian noise
            \item Salt and pepper noise
        \end{itemize}
        \item Temporal noise:
        \begin{itemize}
            \item Locally swapping randomly chosen frames with its neighbor
            \item Globally swapping random frames
            \item Interleaving frames
            \item Switching to different video in middle.
        \end{itemize}
    \end{itemize}

    \subsubsection{Appendix B: Effect of Sample size on FVD}\label{subsubsec:FVD_A_new_Metric_for_Video_Generation:appendix-b:-effect-of-sample-size-on-fvd}

    \subsubsection{Appendix C: Human Evaluation}\label{subsubsec:FVD_A_new_Metric_for_Video_Generation:appendix-c:-human-evaluation}
    \begin{itemize}
        \item Models:
        \begin{itemize}
            \item CDNA
            \item SV2P
            \item SVG-FP
            \item SAVP
        \end{itemize}
        \item BAIR data set.
        \item 3000 models by varying hyperparameters.
        \item 2 context frames, 14 predicted frames.
        \item PSNR and SSIM are computed on 100 videos and the best score is chosen.
        \item 256 videos to compute FVD
        \item \textbf{One Metric Equal}: For a given metric, 10 models are chosen which cannot be distinguished by this metric, which are at 75\% good.
        Videos of these models are evaluated on other metrics and also with subjective evaluation.
        \item \textbf{One Metric Spread}: For a given metric, 10 models are chosen with scores between 10\% to 90\% percentile.
        Videos of these models are evaluated on other metrics and also with subjective evaluation.
        \item Human raters are showed 2 videos and then asked to identify which looks better or are they same.
        3 raters for each pair.
    \end{itemize}
    \newpage


    \section{Reasoning about Physical Interactions with Object-Oriented Prediction and Planning (O2P2)}\label{sec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2)}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Paper: \href{https://arxiv.org/abs/1812.10972}{arXiv}, \href{https://openreview.net/forum?id=HJx9EhC9tQ&noteId=Bkgyox9DnQ}{OpenReview}
        \item Authors: Michael Janner, Sergey Levine, William T Freeman, Joshua B Tenenbaum, Chelsea Finn, Jiajun Wu
        \item Institutions: UC Berkeley, MIT
        \item Project website: \url{https://people.eecs.berkeley.edu/~janner/o2p2/}
        \item Code: \href{https://github.com/JannerM/o2p2}{Generating training data}
        \item Published in: ICLR 2019
        \item Citations: 31 (As of 24-06-2020)
    \end{itemize}

    \subsection*{Abstract}
    \begin{itemize}
        \item O2P2 model jointly learns
        \begin{itemize}
            \item A perception function to map from image observations to object representations
            \item A pairwise physics interaction function to predict the time evolution of a collection of objects
            \item A rendering function to map objects back to pixels
        \end{itemize}
    \end{itemize}

    \subsection{Introduction}\label{subsec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2):introduction}
    \begin{itemize}
        \item Given a scene, can you describe how we arrived at the scene?
        Which objects were moved first and which were moved later?
        \item O2P2: Object-Oriented Prediction and Planning: we train an object representation suitable for physical interactions without supervision of object attributes.
        \item We evaluate our learned model not only on the quality of its predictions, but also on its ability to use the learned representations for tasks that demand a sophisticated physical understanding.
    \end{itemize}

    \subsection{Object-Oriented Prediction and Planning (O2P2)}\label{subsec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2):o2p2}
    \begin{itemize}
        \item O2P2 consists of three components, which are trained jointly
        \begin{itemize}
            \item A perception module that maps from an image to an object encoding.
            The perception module is applied to each object segment independently.
            \item A physics module to predict the time evolution of a set of objects.
            We formulate the engine as a sum of binary object interactions plus a unary transition function.
            \item A rendering engine that produces an image prediction from a variable number of objects.
            We first predict an image and single-channel heatmap for each object.
            We then combine all of the object images according to the weights in their heatmaps at every pixel location to produce a single composite image.
        \end{itemize}
    \end{itemize}

    \subsubsection{Perception Module}\label{subsubsec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2):perception-module}

    \subsubsection{Physics Module}\label{subsubsec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2):physics-module}
    \begin{itemize}
        \item $\bar{o}_k = f_{trans}(o_k) + \sum_{j \neq k} f_{interact}(o_k,o_j) + o_k$
        \item \textcolor{blue}{\textbf{Idea!
        May be this Physics module can be used to validate laws of physics while assessing a video}}
        \item \textcolor{red}{\textbf{Question: Here we consider only effect of one object's movement on another object.
        What if there is transitive effect?
        Movement of object  $A$ affects object $B$ which in turn affects object $C$.
        How is this captured?}}
        \item \textbf{They talk about simplifying the problem by considering only action planning instead of physical interactions.
        Did not understand what that is}
    \end{itemize}

    \subsubsection{Rendering Engine}\label{subsubsec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2):rendering-engine}
    \begin{itemize}
        \item We train such that lower heatmap values are used for nearer objects.
        No true depth-maps are used while training.
    \end{itemize}

    \subsubsection{Learning Object Representations}\label{subsubsec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2):learning-object-representations}
    \begin{itemize}
        \item $S_0 - \textrm{Segmented Image}$, $\textbf{O}  = f_{percept}(S_0)$, $\bar{\textbf{O}} = f_{physics}(\textbf{O})$, $\hat{I}_0 = f_{render}(\textbf{O})$, $\hat{I}_1 = f_{render}(\bar{\textbf{O}})$
        \item We compare each image prediction $\hat{I}_t$ to its ground-truth counterpart using both $\mathcal{L}_2$ distance and a perceptual loss $\mathcal{L}_{\textrm{VGG}}$.
        \item Perceptual Loss: $\mathcal{L}_2$ distance is feature space of pretrained VGG network.
        \item Losses for individual engines:
        \begin{itemize}
            \item Perception Module: Reconstruction of $I_0$: $\mathcal{L}_{\textrm{percept}} = \mathcal{L}_2(\hat{I}_0, I_0) + \mathcal{L}_{\textrm{VGG}}(\hat{I}_0, I_0)$
            \item Physics Engine: Reconstruction of $I_1$: $\mathcal{L}_{\textrm{physics}} = \mathcal{L}_2(\hat{I}_1, I_1) + \mathcal{L}_{\textrm{VGG}}(\hat{I}_1, I_1)$
            \item Rendering Engine: Reconstruction of both $I_0$ and $I_1$: $\mathcal{L}_{\textrm{render}} = \mathcal{L}_{\textrm{percept}} + \mathcal{L}_{\textrm{physics}}$
        \end{itemize}
    \end{itemize}

    \subsubsection{Planning with Learned Models}\label{subsubsec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2):planning-with-learned-models}
    \begin{itemize}
        \item High level algorithm:
        \begin{itemize}
            \item Given the segmented goal image, perception module extracts object representations ($O^{\textrm{goal}}$)
            \item Sample actions (initial frame)
            \item For each sample action (initial frame), extract object representations using perception module.
            Apply physics engine to predict the end-result.
            Compare end-goal with end-result using $\mathcal{L}_2$ distance between object representations (not $\mathcal{L}_2$ in pixel domain).
            \item Choose the sampled action with least $\mathcal{L}_2$ distance.
            Execute the action in MoJoCo.
            \item Repeat as many actions as the number of objects in the given goal image.
        \end{itemize}
    \end{itemize}

    \subsection{Experimental Evaluation}\label{subsec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2):experimental-evaluation}
    Goal:
    \begin{itemize}
        \item Can O2P2 reason physical interactions in an actionable way
        \item Does object factorization helps in video prediction i.e.\ does it perform better than black-box video prediction?
        \item Is object factorization useful, even without supervision?
    \end{itemize}

    \subsubsection{Image Reconstruction and Prediction}\label{subsubsec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2):image-reconstruction-and-prediction}
    \begin{itemize}
        \item \textbf{MuJoCo} simulator is used to generate training data.
        \item 60000 training images: Only initial and final frames.
    \end{itemize}

    \subsubsection{Building Towers}\label{subsubsec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2):building-towers}
    \begin{itemize}
        \item Compared with No Physics Ablation, SAVP, Oracle (pixels) and Oracle(objects) models.
        \item Evaluation metric:
        \begin{itemize}
            \item Simulator: Object error is computed in terms of position, identity and color and then thresholded to get a binary value.
            Then accuracy is computed as fraction of goals achieved.
            Only relative ordering matters.
            Metric values differ based on threshold.
            \item Robotic arm: Percentage of goal configurations built.
        \end{itemize}
        \item SAVP model failed in stacking objects to build towers.
    \end{itemize}

    \subsubsection{The importance of understanding physics}\label{subsubsec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2):the-importance-of-understanding-physics}
    \begin{itemize}
        \item Although the model is never trained to produce valid action decisions, the planning procedure selects a physically stable sequence of actions.
    \end{itemize}

    \subsubsection{Transfer to Robotic Arm}\label{subsubsec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2):transfer-to-robotic-arm}
    \begin{itemize}
        \item Given a goal image, they make a Sawyer robotic arm place the appropriate objects at appropriate places in appropriate sequence
        \item Two layer MLP to map actions to object representations: $o_m = f_{\textrm{embedder}}(a_m)$
    \end{itemize}

    \subsection{Related Work}\label{subsec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2):related-work}
    \begin{itemize}
        \item This item is in between two kinds of works
        \begin{itemize}
            \item Regid notion of object representation via supervision.
            \item No scene factorization into objects.
        \end{itemize}
        \item We show that a model capable of predicting and reasoning about physical phenomena can also be employed for decision making.
        \item This model achieves substantially better results at tasks that require building structures out of blocks.
    \end{itemize}

    \subsection{Conclusion}\label{subsec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2):conclusion}

    \subsection{My summary}\label{subsec:Reasoning_about_Physical_Interactions_with_Object_Oriented_Prediction_and_Planning_(O2P2):my-summary}
    Given a goal image made up of blocks, O2P2 predicts the action sequences to be performed to achieve the goal image.

    For every action, multiple initial frames are sampled and fed to physics engine.
    It outputs the steady state result of that action.
    This result is compared to end-goal image in object representation space using $\mathcal{L}_2$ distance and the action that has least distance is picked.
    This action is given to MoJoCo simulator and the result is obtained.
    This is repeated for as many number of actions as the number of objects in the end-goal image.

    The claim is that physics engine takes care of stability and in turn leads to appropriate sequence of actions.
    They conclude that for building towers O2P2 (applying physics on object representations) is better than object agnostic models (applying physics directly in pixel space).
    \newpage


    \section{Time Agnostic Prediction (Conference Paper - ICLR 2019)}\label{sec:Time_Agnostic_Prediction_(Conference_Paper_ICLR_2019)}

    \newpage


    \section{Deep Compressed Sensing}\label{sec:Deep_Compressed_Sensing}
    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Deep_Compressed_Sensing:introduction}
    \begin{itemize}
        \item Compressed Sensing (CS) provides a framework that separates encoding and decoding into independent measurement and reconstruction processes.
        \item Autoencoders are trained end-to-end.
        CS reconstructs via online optimization.
        \item Neural Networks are trained from scratch for both measuring and online reconstruction.
        \item Contributions:
        \begin{itemize}
            \item How to train Deep Nets within CS framework
            \item Meta-learned reconstruction process is more accurate and faster
            \item New GAN based on latent optimzation
            \item Extension of this framework to semi-supervised GANs. Gives meaningful latent spaces.
        \end{itemize}
    \end{itemize}

    \subsection{Background}\label{subsec:Deep_Compressed_Sensing:background}

    \subsubsection{Compressed Sensing}\label{subsubsec:Deep_Compressed_Sensing:cs}
    \begin{itemize}
        \item $\textbf{m} = \textbf{Fx} + \eta $\\
        where \\
        x is the signal \\
        F is CxD measurement matrix.
        Typically wide/fat (C \textless \textless D) \\
        $\eta$ is measurement noise \\

        \item Restricted Isometry Property (RIP)
        \[(1-\delta) \Vert {x_1 - x_2} \Vert_2^2 \leq \Vert {F(x_1 - x_2)} \Vert_2^2 \leq (1+\delta) \Vert {x_1 - x_2} \Vert_2^2 \]
        $\delta \in (0,1)$ is a small constant. \\
        It states that projection from \textbf{F} preserves the distance between 2 signals, bounded by factors of (1-$\delta$) and (1+$\delta$) \\
        This property holds with high probability for various random matrices \textbf{F} and sparse signals $x$

        \item Compressed Sensing guarantees that minimizing the mesurement error under the constraint that $x$ is sparse, leads to accurate reconstruction $\hat{x} \approx x$ with high probability.
        \[\hat{x} = \argmin_x \Vert \textbf{m - Fx} \Vert_2^2\]

        \item The contraint that $x$ being sparse can be replaced by sparsity in a set of basis $\phi$ i.e. $z$ is sparse, but $x = \phi z$ need not be.
        Measurement function is applied on $x = \phi z$.
        In other words, the signal should be sparse in transform domain.
    \end{itemize}

    \subsubsection{Compressed Sensing using Generative Models (CSGM)}\label{subsubsec:Deep_Compressed_Sensing:csgm}
    \begin{itemize}
        \item Sparse basis like Fourier or Wavelet restricts to very few data distributions.
        To relax this, a Generator (neural network) is used map a latent representation $z$ to signal space $x$:
        \[x = G_\theta(z)\]

        \item This implicity constrains $x$ in a low-dimensional manifold.
        This constraint provides Set-Restricted Eigenvalue Condition (S-REC) with random matrices.
        \item With this condition, low reconstruction error with high probability can be achieved similar to CS\@.
        \begin{align*}
            \hat{z} &= \argmin_z E_\theta(m,z) \\
            E_\theta(m,z) &= \Vert {m - F G_\theta(z)} \Vert_2^2 \\
            \hat{x} &= G_\theta(z) \quad \quad \quad \quad \quad \quad \textrm{ is the reconstructed signal} \\
        \end{align*}

        \item Compared to previous one, where the optimization is directly in signal space $x$, here the optimization is in latent space $z$\\
        \item $E_\theta$ is intractable.
        Hence Gradient Descent is used.
        \begin{gather*}
            \hat{z} \sim p_z(z)\\
            \hat{z} \leftarrow \hat{z}- \alpha \frac{\partial E_\theta(m,z)}{\partial z} \bigg|_{z=\hat{z}}\\
        \end{gather*}

        \item $T$ steps of Gradient Descent are used.
        Typically thousands of gradient descent steps are required with several restarts to get a sufficiently good $\hat{z}$

        \item Disadvantages of CSGM
        \begin{itemize}
            \item Optimization is slow.
            \item Random measurement matrices are sub-optimal.
        \end{itemize}
    \end{itemize}

    \subsubsection{Model Agnostic Meta Learning (MAML)}\label{subsubsec:Deep_Compressed_Sensing:maml}
    \begin{itemize}
        \item Train a generic model, which can be adapted to any new task with minimal training.
        \item Even if the loss function is highly non-convex, by back-propagating thru gradient-descent process, only few gradient steps are enough to adapt to new tasks.
    \end{itemize}

    \subsubsection{Generative Adversarial Networks}\label{subsubsec:Deep_Compressed_Sensing:gans}
    \begin{itemize}
        \item In most GAN models, discriminators become useless after training.
        Here, discriminators are used to move latent representations to areas more likely to generate realistic images.
    \end{itemize}

    \subsection{Deep Compressed Sensing}\label{subsec:Deep_Compressed_Sensing:dcs}
    Two stages:
    \begin{itemize}
        \item Combine Meta Learning with CSGM
        \item Measurement matrices are replaced by functions (DeepNets)
    \end{itemize}

    \subsubsection{Compressed Sensing with Meta Learning}\label{subsubsec:Deep_Compressed_Sensing:csml}
    \begin{itemize}
        \item Refer to Algorithm 1
        \item Meta Learning is used in Latent Optimization
        \item Model parameters and latent variables are trained to minimize measurement error
        \[\min_\theta \mathcal{L}_G = \mathbb{E}_{x_i \sim p_{data}(x)} \left[ E_\theta(m_i, \hat{z}_i) \right]\]
        \item Gradient of $E_\theta$ is not stochastic gradient since there is only one sample of z
        \item Online optimization is over latent variables rather than parameters.
        Since latent variables are much fewer, update is quicker.
        \item Enforcing RIP is equivalent to minimizing the below
        \[\mathcal{L}_F = \mathbb{E}_{x_1,x_2}\left[(\Vert{F(x_1-x_2)}\Vert_2 - \Vert{x_1-x_2}\Vert_2)^2\right]\]
    \end{itemize}

    \subsubsection{Deep Compressed Sensing with Learned Measurement Function}\label{subsubsec:Deep_Compressed_Sensing:dcs-learned-measurement}

    \paragraph{Learning Measurement Function}
    \begin{itemize}
        \item Refer to Algorithm 2
        \item Measurement Matrix \textbf{F} is made trainable $F_\phi(x)$
        \item All equations are similar
        \begin{align*}
            E_\theta(m,z) &= \Vert {m-F_\phi(G_\theta(z))} \Vert_2^2 \\
            \mathcal{L}_G &= \mathbb{E}_{x_i \sim p_{data}(x)} \left[ E_\theta(m_i, \hat{z}_i) \right] \\
            \mathcal{L}_F &= \mathbb{E}_{x_1,x_2}\left[(\Vert{F_\phi(x_1-x_2)}\Vert_2 - \Vert{x_1-x_2}\Vert_2)^2\right] \\
            \min_{\theta, \phi} & (\mathcal{L}_G + \mathcal{L}_F)
        \end{align*}
    \end{itemize}

    \paragraph{Generalised CS 1: CS-GAN}
    \begin{itemize}
        \item 1D measurement i.e.\ Measurement Function encodes how likely the input data is real or fake
        \item One way to formulate this is to train measurement function $F_\phi$ using the below loss
        \begin{align*}
            \mathcal{L}_F =
            \begin{cases}
                \Vert {F_\phi(x) - 1} \Vert_2^2 \quad \quad &x \sim p_{\textrm{data}}(x) \\
                \Vert {F_\phi(\hat{x})} \Vert_2^2 \quad \quad \quad \quad &\hat{x} \sim G_\theta(\hat{z}), \forall \hat{z} \\
            \end{cases}
        \end{align*}
        This becomes similar to Least-Squares GAN
        \item To get original GAN, set T=0 (in algorithm 2) and use a binary classifier $D_\phi$ as measurement function which gives the probability that $x$ comes from the dataset.
        This measurement function is equivalent to Discriminator.
        We need to use cross-entropy loss rather than squared loss
        \[\mathcal{L}_F = t(x)\ \ln[D_\phi(x)] + (1-t(x))\ \ln[1-D_\phi(x)]\]
        where
        \[t(x) =
        \begin{cases}
            1 \quad x \sim p_{data}(x) \\
            0 \quad x \sim G_\theta(z),\ \forall z
        \end{cases}
        \]
    \end{itemize}
    \newpage


    \section{ChainQueen: A Real-Time Differentiable Physical Simulator for Soft Robotics}\label{sec:ChainQueen_A_Real_Time_Differentiable_Physical_Simulator_for_Soft_Robotics}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Paper: \href{https://arxiv.org/abs/1810.01054}{arXiv}, \href{https://ieeexplore.ieee.org/abstract/document/8794333}{IEEE Xplore}
        \item Authors: Joshua Tenenbaum, William Freeman, Jiajun Wu
        \item Institutions: MIT
        \item Project website:
        \item Code: \url{https://github.com/yuanming-hu/ChainQueen}
        \item Published in: ICRA 2019
        \item Presentation: \href{https://www.youtube.com/watch?v=4IWD4iGIsB4}{video}
        \item Citations: 22 (As of 24-06-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:ChainQueen_A_Real_Time_Differentiable_Physical_Simulator_for_Soft_Robotics:introduction}
    \begin{itemize}
        \item Motivation: Differentiable Physical Simulators enable the use of gradient-based optimizers for inverse-physics problems.
        \item Contributions:
        \begin{itemize}
            \item Differentiable Physical Simulator for deformable objects.
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:ChainQueen_A_Real_Time_Differentiable_Physical_Simulator_for_Soft_Robotics:related-work}

    \subsection{Forward Simulation and Back Propagation}\label{subsec:ChainQueen_A_Real_Time_Differentiable_Physical_Simulator_for_Soft_Robotics:forward-simulation-and-back-propagation}
    \begin{itemize}
        \item Loss Functions:
    \end{itemize}

    \subsection{Evaluation}\label{subsec:ChainQueen_A_Real_Time_Differentiable_Physical_Simulator_for_Soft_Robotics:evaluation}
    \begin{itemize}
        \item Efficiency: Time taken per frame for a given number of particles.
        \item Accuracy: Relative error in forward simulation and gradient.
        \item Evaluation Metrics:
    \end{itemize}

    \subsection{My Summary}\label{subsec:ChainQueen_A_Real_Time_Differentiable_Physical_Simulator_for_Soft_Robotics:my-summary}
    \begin{itemize}
        \item
    \end{itemize}
    \newpage


    \section{High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks}\label{sec:High_Fidelity_Video_Prediction_with_Large_Stochastic_Recurrent_Neural_Networks}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Ruben Villegas
        \item Institutions: University of Michigan, Google Research, Adobe Research
        \item Project website: \url{https://sites.google.com/view/videopredictioncapacity}
        \item Code:
        \item Published in: NIPS 2019
        \item Citations: 1 (As of 09-04-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:High_Fidelity_Video_Prediction_with_Large_Stochastic_Recurrent_Neural_Networks:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:High_Fidelity_Video_Prediction_with_Large_Stochastic_Recurrent_Neural_Networks:related-work}

    \subsection{Method}\label{subsec:High_Fidelity_Video_Prediction_with_Large_Stochastic_Recurrent_Neural_Networks:method}
    \begin{itemize}
        \item Loss Functions:
    \end{itemize}

    \subsection{Experiments}\label{subsec:High_Fidelity_Video_Prediction_with_Large_Stochastic_Recurrent_Neural_Networks:experiments}
    \begin{itemize}
        \item Datasets: Towel pick, Human 3.6M, KITTI
        \item Baselines:
        \item Evaluation Metrics:
    \end{itemize}
    \newpage


    \section{Large-Scale Study of Perceptual Video Quality (LIVE VQC Database)}\label{sec:Large_Scale_Study_of_Perceptual_Video_Quality_(LIVE_VQC_Database)}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Zeina Sinno, Alan Bovik
        \item Institutions: UT Austin
        \item Project website: \url{http://live.ece.utexas.edu/research/LIVEVQC/index.html}
        \item Code:
        \item Published in: TIP 2019 Feb
        \item Citations: 10 (As of 11-02-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Large_Scale_Study_of_Perceptual_Video_Quality_(LIVE_VQC_Database):introduction}
    \begin{itemize}
        \item Large database of 585 videos, 80 videographers, 43 different models, 101 unique devices, 205000 opinion scores.
        \item Has real world distortions rather than synthesized distortions.
        \item Most existing video quality assessment databases offer only very limited varieties of video content, shot by only a few users, thereby constraining the ability of learned VQA models trained on them to generalize to diverse contents, levels of videographic expertise, and shooting styles.
        \item Previous study [20] did not take care of video stalling or frame freezing during subjective study.
        \item In KoNViD-1k database, subject was asked to rate any number of videos within range 10--550.
        This leads to subject fatigue.
        \item Contributions:
        \begin{itemize}
            \item A new robust framework for conducting crowd-sourced video quality studies and for collecting video quality scores in AMT\@.
            \item New LIVE VQC database.
        \end{itemize}
    \end{itemize}

    \subsection{LIVE Video Quality Challenge (LIVE-VQC) Database}\label{subsec:Large_Scale_Study_of_Perceptual_Video_Quality_(LIVE_VQC_Database):database}
    \begin{itemize}
        \item Features:
        \begin{itemize}
            \item 585 videos
            \item 80videographers
            \item 43 different models
            \item 101 unique devices
            \item Min 10s video
            \item No post processing (like Snapchat)
            \item No single contributor captured more than 9\% of the videos.
            \item Contributors span age range of 11--65 years.
            \item No distortion labels to videos
        \end{itemize}
        \item Table of different video resolutions
        \item All portrait videos of resolutions $1080 \times 1920$, $2160 \times 3840$ and $720 \times 1080$ were downscaled using bicubic interpolation to $404 \times 720$.
        This is done so that all these videos can be displayed at native resolutions for subjective study.
    \end{itemize}

    \subsection{Testing Methodology}\label{subsec:Large_Scale_Study_of_Perceptual_Video_Quality_(LIVE_VQC_Database):testing-methodology}
    \begin{itemize}
        \item Single Stimulus study
        \item 50 videos in a session: 7 training and 43 testing.
        \item 43 test videos include:
        \begin{itemize}
            \item 4 distorted videos from LIVE IQA (golden videos).
            \item 31 random videos from the new database.
            \item 4 repeated videos from above 31
            \item 4 videos are rated by all subjects.
        \end{itemize}
        \item Initial position of cursor was randomized.
        \item Subject Features:
        \begin{itemize}
            \item 4776 subjects
            \item 56 countries
            \item 1 dollar per subject
            \item 205000 opinion scores
            \item 205 opinion scores per video (after rejection)
        \end{itemize}
    \end{itemize}

    \subsection{Subjective Data Processing and Results}\label{subsec:Large_Scale_Study_of_Perceptual_Video_Quality_(LIVE_VQC_Database):processing}
    \begin{itemize}
        \item Subject Consistency is checked by dividing scores on a video into 2 group and checking correlation between the means of the thwo groups.
    \end{itemize}

    \subsection{Performance of Video Quality Predictors}\label{subsec:Large_Scale_Study_of_Perceptual_Video_Quality_(LIVE_VQC_Database):performance-vqa}
    \begin{itemize}
        \item For models that require training, 5 fold cross validation technique was used.
        The scores from each fold are aggregated.
        Scatter plot of NIQE, VIIDEO, BRISQUE and V-BLIINDS against MOS scores.
        VIIDEO correlated poorly.
        \item For evaluating models, 100 splits were used.
        Median PLCC, SROCC and RMSE values are reported.
    \end{itemize}
    \newpage


    \section{Predicting the Quality of Images Compressed After Distortion in Two Steps}\label{sec:Predicting_the_Quality_of_Images_Compressed_After_Distortion_in_Two_Steps}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Xiangxu Yu, Praful Gupta, Alan Bovik
        \item Institutions: UT Austin, Netfilx
        \item Project website: \url{http://live.ece.utexas.edu/research/twostep/index.html}
        \item Code: \url{https://github.com/xiangxuyu/2stepQA}
        \item Published in: TIP 2019
        \item Citations: 1 (As of 08-02-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Predicting_the_Quality_of_Images_Compressed_After_Distortion_in_Two_Steps:introduction}
    \begin{itemize}
        \item Pictures captured by non profession users are already of lower quality.
        Quality degrades more on compressing them.
        \item FR metrics don't consider the already poor quality of reference images.
        NR metrics don't perform so well.
        Since there is a reference available, it can be used to get better predictions of quality.
        \item Both Full Reference (FR) and Reduced Reference (RR) IQA models will be referred to as R IQA models.
    \end{itemize}

    \subsection{Related Work}\label{subsec:Predicting_the_Quality_of_Images_Compressed_After_Distortion_in_Two_Steps:related-work}

    \subsection{Two Step IQA Model}\label{subsec:Predicting_the_Quality_of_Images_Compressed_After_Distortion_in_Two_Steps:two-step-iqa-model}
    \begin{itemize}
        \item R IQA models predict quality of distorted images by making a perceptual comparison of it with a reference image.
        \item R IQA model is actually a perceptual fidelity measure.
        \item R IQA models only provide relative image quality scores.
        If the reference image itself is of lower quality, that is not taken into account.
        By augmenting a NR IQA score with R IQA score, a better prediction of perceptual distance from the natural image space can be made.
        \item Generally, a two step model should fulfill three important properties:
        \begin{itemize}
            \item If compression does not occur, or has an imperceptible effect on quality, then the two-step model should report the innate source (reference) image quality.
            \item If the source is pristine, then the two-step model should accurately predict the effect of compression on perceived quality.
            \item If the source is already distorted and then compressed with perceptual loss, then the two-step model should yield a better prediction than either the R and NR components applied on the compression image.
        \end{itemize}
        \item 2 step model:
        \begin{itemize}
            \item Predict the quality of reference image using a NR IQA model (prior quality) - $Q_{\textrm{NR}}$.
            \item Predict the quality of distorted image w.r.t.\ reference image using a R IQA model (conditional quality) - $Q_{\textrm{R}}$.
            \item Re-map the scores to same interval - $Q'_{\textrm{NR}}$ and $Q'_{\textrm{R}}$.
            \item Take the product of the two scores - $Q'_{\textrm{NR}} \cdot Q'_{\textrm{R}}$
        \end{itemize}
        \item Example: Using MS-SSIM as R IQA measure and NIQE as NR IQA measure, we get
        \[ Q_{\textrm{2stepQA}} = \textrm{MS-SSIM} \cdot \left( 1 - \frac{\textrm{NIQE}}{\alpha} \right) \]
        $ \alpha $ is a scaling constant and is set to 100. \\
        2stepQA is robust over a wide range of the values of $\alpha \in [50, 150] $.
        \item Logistic Remapping: To preserve monotonicity, allow for generalizability, and to scale the scores to either [0, 1] or the MOS range, a simple four-parameter logistic mapping is used.
        \[ Q' = \beta_2 + \frac{\beta_1 - \beta_2}{1 + e^{- \left( Q - \beta_3 / \vert \beta_4 \vert \right)}} \]
        The parameters $\beta$ can be effectively determined by using the subjective data from one or more IQA databases.
        \item If a model is trained on MOS scores, it need not be remapped.
        \item Instead of simple product, a general model can have exponentially weighted product of the two scores:
        \[ Q_\textrm{G} = \left(Q'_{\textrm{NR}}\right)^\gamma \cdot \left(Q'_{\textrm{R}}\right)^{1-\gamma} \qquad \textrm{where} \quad \gamma \in [0,1] \]
    \end{itemize}

    \subsection{A new Distorted-Then-Compressed Image Database}\label{subsec:Predicting_the_Quality_of_Images_Compressed_After_Distortion_in_Two_Steps:new-image-database}
    \begin{itemize}
        \item The new database is called `LIVE Wild Compressed Picture Quality Database'.
        \item 80 images are chosen from LIVE In the Wild Challenge IQA Database.
        These images are subjected to JPEG compression at 4 different levels.
        Thus a there are a total of 400 images.
        \item Subjective Study: 6 out of 29 subjects were outliers.
        \item Subject Consistency Analysis: The subjects are divided into two disjoint equal groups and MOS is computed on each image for both the groups.
        SROCC is computed between the scores and this is repeated 25 times.
        SROCC was found to be 0.9805.
        \item Box plot of MOS at different compression levels is plotted.
        \item A line graph of MOS scores for each content image at different compression levels is plotted
    \end{itemize}

    \subsection{Performance Evaluation}\label{subsec:Predicting_the_Quality_of_Images_Compressed_After_Distortion_in_Two_Steps:performance-evaluation}
    \begin{itemize}
        \item Since DMOS doesn't represent actual quality, MOS is used here.
        \item LCC and SROCC are used for evaluation.
        \item Predicted IQA scores were passed through a logistic non-linearity before computing the LCC measure.
        \item 80\% training sets and 20\% testing sets (even for non-trainable models).
        1000 random such splits.
        \item Statistical Significance Test:
        \begin{itemize}
            \item Nonparametric Wilcoxon Rank Sum Test is used.
            \item SROCC scores over 1000 splits are used.
            \item Null hypothesis is that the median SROCC of the two algorithms is same.
            \item 95\% significance level is used.
        \end{itemize}
        \item Box plot of LCC and SROCC over 1000 splits, showing median value and standard deviation, is plotted.
        \item To test where 2stepQA beats MS-SSIM:
        \begin{itemize}
            \item Using NIQE, reference images (80) are divided into two equal groups, one with higher quality images and other with lower quality images.
            The corresponding compressed images are put with their respective reference images.
            \item MS-SSIM and 2stepQA both correlated similarly on the subset of high quality reference images.
            \item However, on the subset of poor quality reference images, 2stepQA significantly outperformed MS-SSIM\@.
            This is because of the contribution of the NR component.
        \end{itemize}
        \item To highlight the importance of accurate NR algorithms, 2stepQA is also done with using actual MOS of the reference images as the NR score.
        \item In the general two step model, $\gamma$ represents the relative contribution of NR component.
        \item For a fixed NR model, $\gamma$ is higher for high performing R model and vice-versa.
        \item The choice of NR algorithm doesn't influence the performance of two step model, as much as the choice of R algorithm.
        \item Generalized models achieved nearly optimal performance for fixed $\gamma=0.5$ as well (compared to the case when the model is optimized for $\gamma$).
        \item Performance of two step model as $\alpha, \gamma$ varies is plotted (two different plots).
        \item When reference images are of high quality, MS-SSIM performs better than 2stepQA, but 2stepQA is not far behind.
        But NIQE does not do so well.
        \item Note that predicted score of 2stepQA model will be close to 1 only when both the reference image and the compressed image are of high quality.
        \item Different ways of combining R and NR scores have been tried, but multiplication gave best result.
        \item The problem can be viewed as prediction R quality after compression, given the NR quality measuremtn before compression
    \end{itemize}
    \newpage


    \section{Event-driven Video Frame Synthesis}\label{sec:Event_driven_Video_Frame_Synthesis}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: North Western University
    \end{itemize}

    \subsection*{Abstract}
    \begin{itemize}
        \item Code and demo: \url{https://github.com/winswang/int-event-fusion/tree/win10}
    \end{itemize}

    \subsection{My Summary}\label{subsec:Event_driven_Video_Frame_Synthesis:my-summary}
    \begin{itemize}
        \item Synthesizes intermdediate frames to produce high frame rate video that can capture high speed events
        \item Denoises videos at the end
        \item Video Prediction: Based on past 2 frames and future events, predicts the next frame.
        \item Evaluation metrics: PSNR, SSIM
    \end{itemize}

    \newpage


    \section{From Here to There: Video Inbetweening Using Direct 3D Convolutions}\label{sec:From_Here_to_There_Video_Inbetweening_Using_Direct_3D_Convolutions}
    \subsection*{Abstract}
    \begin{itemize}
        \item Datasets: BAIR Robot pushing, KTH, UCF101
        \item Generates 14 in-between frames
        \item Conditioned on only 1 past and 1 future frame.
        Total 16 frames.
        \item Frames downsampled and cropped to 64x64
    \end{itemize}
    \newpage


    \section{Physics-as-Inverse-Graphics: Joint Unsupervised Learning of Objects and Physics from Video}\label{sec:Physics_as_Inverse_Graphics_Joint_Unsupervised_Learning_of_Objects_and_Physics_from_Video}
    \subsection*{Abstract}
    \begin{itemize}
        \item Datset: 2D circles, digits moving.
    \end{itemize}
    \newpage


    \section{Scaling Autoregressive Video Models}\label{sec:Scaling_Autoregressive_Video_Models}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Dirk Weissenborn
        \item Institutions: Google Research
        \item Project website: \url{https://sites.google.com/view/video-transformer-samples}
        \item Code:
        \item Published in: 2019
        \item Citations: 2 (As of 20-10-2019)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Scaling_Autoregressive_Video_Models:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
        \item Auto-regressive models generate videos pixel by pixel.
    \end{itemize}

    \subsection{Related Work}\label{subsec:Scaling_Autoregressive_Video_Models:related-work}

    \subsection{Video Transformer}\label{subsec:Scaling_Autoregressive_Video_Models:video-transformer}
    \begin{itemize}
        \item Loss Functions:
    \end{itemize}

    \subsection{Experiments}\label{subsec:Scaling_Autoregressive_Video_Models:experiments}
    \begin{itemize}
        \item Datasets: BAIR, Kinetics-600, Moving MNIST, PUSH
        \item Baselines:
        \item Evaluation Metrics: SSIM, FVD
    \end{itemize}
    \newpage


    \section{VideoFlow: A Flow-Based Generative Model for Video}\label{sec:VideoFlow_A_Flow_Based_Generative_Model_for_Video}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Manoj Kumar, Mohammad Babaeizadeh, Chelsea Finn, Sergey Levine, Durk Kingma
        \item Institutions: Google Brain, University of Illinois
        \item Project website: \url{https://sites.google.com/view/videoflow/home}
        \item Code: \url{https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/video/next_frame_glow.py}
        \item Published in: 2019
        \item Citations: 15 (As of 22-10-2019)
    \end{itemize}

    \subsection*{Abstract}
    \begin{itemize}
        \item A model for video prediction based on normalizing flows, which allows for direct optimization of the data likelihood, and produces high-quality stochastic predictions.
    \end{itemize}

    \subsection{Introduction}\label{subsec:VideoFlow_A_Flow_Based_Generative_Model_for_Video:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:VideoFlow_A_Flow_Based_Generative_Model_for_Video:related-work}

    \subsection{Preliminaries: Flow-Based Generative Models}\label{subsec:VideoFlow_A_Flow_Based_Generative_Model_for_Video:flow-based-gms}
    \begin{itemize}
        \item Loss Functions: log-likelihood
    \end{itemize}

    \subsection{Proposed Architecture}\label{subsec:VideoFlow_A_Flow_Based_Generative_Model_for_Video:proposed-architecture}

    \subsection{Quantitative Experiments}\label{subsec:VideoFlow_A_Flow_Based_Generative_Model_for_Video:quantitative-experiments}
    \begin{itemize}
        \item Datasets: Stochastic Movement Dataset, BAIR
        \item Baselines: SAVP-VAE, SV2P
        \item Evaluation Metrics: PSNR, SSIM, VGG cosine similarity (best out of 100)
        \item On a temporal patch of 4 frames, the log-likehood of 4th frame given the context of 3 previous frames is maximized.
    \end{itemize}

    \subsection{Qualitative Experiments}\label{subsec:VideoFlow_A_Flow_Based_Generative_Model_for_Video:qualitative-experiments}
    \begin{itemize}
        \item 100 frames are predicted.
    \end{itemize}
    \newpage


    \section{Probabilistic Video Prediction from Noisy Data with a Posterior Confidence (BP-Net)}\label{sec:Probabilistic_Video_Prediction_from_Noisy_Data_with_a_Posterior_Confidence_(BP_Net)}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Authors: Yunbo Wang, Joshua B. Tenenbaum
        \item Institutions: Stanford, MIT
        \item Project website:
        \item Code:
        \item Published in: CVPR 2020
        \item Citations: 0 (As of 09-04-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Probabilistic_Video_Prediction_from_Noisy_Data_with_a_Posterior_Confidence_(BP_Net):introduction}
    \begin{itemize}
        \item Motivation:
        \begin{itemize}
            \item Precipitation forecasting
            \item Traffic flows prediction
            \item Robotics
        \end{itemize}
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Probabilistic_Video_Prediction_from_Noisy_Data_with_a_Posterior_Confidence_(BP_Net):related-work}

    \subsection{Method}\label{subsec:Probabilistic_Video_Prediction_from_Noisy_Data_with_a_Posterior_Confidence_(BP_Net):method}
    \begin{itemize}
        \item Loss Functions:
    \end{itemize}

    \subsection{Experiments}\label{subsec:Probabilistic_Video_Prediction_from_Noisy_Data_with_a_Posterior_Confidence_(BP_Net):experiments}
    \begin{itemize}
        \item Datasets: Moving MNIST, KTH
        \item Baselines: DFN, FRNN, PredRNN++, VideoGAN, MCnet, SVG-LP
        \item Evaluation Metrics: MSE, SSIM over 100 predictions (best and worst)
    \end{itemize}
    \newpage


    \section{Learning Human Objectives by Evaluating Hypothetical Behavior}\label{sec:Learning_Human_Objectives_by_Evaluating_Hypothetical_Behavior}
    \subsection*{Paper Details}
    \begin{itemize}
        \item Paper: \href{https://arxiv.org/abs/1912.05652}{arXiv}
        \item Authors: Sergey Levine
        \item Institutions:
        \item Project website:
        \item Code:
        \item Published in: ICML 2020
        \item Citations:  (As of 21-06-2020)
    \end{itemize}

    \subsection*{Abstract}

    \subsection{Introduction}\label{subsec:Learning_Human_Objectives_by_Evaluating_Hypothetical_Behavior:introduction}
    \begin{itemize}
        \item Motivation:
        \item Contributions:
        \begin{itemize}
            \item
        \end{itemize}
    \end{itemize}

    \subsection{Related Work}\label{subsec:Learning_Human_Objectives_by_Evaluating_Hypothetical_Behavior:related-work}

    \subsection{Method}\label{subsec:Learning_Human_Objectives_by_Evaluating_Hypothetical_Behavior:method}
    \begin{itemize}
        \item Loss Functions:
    \end{itemize}

    \subsection{Experiments}\label{subsec:Learning_Human_Objectives_by_Evaluating_Hypothetical_Behavior:experiments}
    \begin{itemize}
        \item Datasets:
        \item Baselines:
        \item Evaluation Metrics:
    \end{itemize}

    \subsection{My Summary}\label{subsec:Learning_Human_Objectives_by_Evaluating_Hypothetical_Behavior:my-summary}
    \begin{itemize}
        \item Setup:
        \begin{itemize}
            \item There is an RL agent that learns to take actions to solve a task.
            \item The rewards to the RL agent is generated by a neural network.
            \item The reward generating neural network is trained with data containing (state, action, reward-label) tuples.
            \item A generator generates trajectories that are shown to user who labels them and then are used to train the reward generating neural network.
        \end{itemize}
        \item The goal of this work is to train a better reward generating neural network.
        \item An acquisition function is used to generate more useful trajectories.
        The trajectories are predicted which
        \begin{itemize}
            \item maximize uncertainty in the reward generated by the neural network.
            \item maximizes the reward.
            \item minimizes the reward.
            \item maximizes the novelty of trajectories (diversity).
        \end{itemize}
        \item Because of this acquisition function, good training data is generated which in turn leads to faster convergence of reward generating neural network.
    \end{itemize}
    \newpage


\end{document}
